<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>单机安装Hbase+Hadoop+Zookeeper</title>
      <link href="/2020/01/14/%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85Hbase-Hadoop-Zookeeper/"/>
      <url>/2020/01/14/%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85Hbase-Hadoop-Zookeeper/</url>
      
        <content type="html"><![CDATA[<h1 id="单机环境安装Zookeeper-Hadoop-Hbase-Phoenix"><a href="#单机环境安装Zookeeper-Hadoop-Hbase-Phoenix" class="headerlink" title="单机环境安装Zookeeper+Hadoop+Hbase+Phoenix"></a>单机环境安装Zookeeper+Hadoop+Hbase+Phoenix</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><blockquote><p><strong>系统环境</strong>：Centos 7.6</p><p><strong>配置映射</strong></p></blockquote><pre class=" language-shell"><code class="language-shell">[root@ java]# vim /etc/hosts</code></pre><p>添加如下配置：</p><pre class=" language-shell"><code class="language-shell"># 文件末尾增加本机IP地址  hadoop001</code></pre><blockquote><p><strong>使用软件版本说明</strong><br>因为Apache的Hadoop版本和Hbase的版本不太稳定所以本文所用的都为CDH版本</p></blockquote><h2 id="一-安装JDK"><a href="#一-安装JDK" class="headerlink" title="一.安装JDK"></a>一.安装JDK</h2><blockquote><p><strong>JDK 版本</strong>：jdk 1.8.0_20</p></blockquote><h3 id="1-1-下载并解压"><a href="#1-1-下载并解压" class="headerlink" title="1.1 下载并解压"></a><a name="1.1 下载并解压">1.1 下载并解压</a></h3><p>在<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="external">官网</a> 下载所需版本的 JDK，这里我下载的版本为<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="external">JDK 1.8</a> ,下载后进行解压：</p><pre class=" language-shell"><code class="language-shell">[root@ java]# tar -zxvf jdk-8u201-linux-x64.tar.gz</code></pre><h3 id="1-2-设置环境变量"><a href="#1-2-设置环境变量" class="headerlink" title="1.2 设置环境变量"></a><a name="1.2 设置环境变量">1.2 设置环境变量</a></h3><pre class=" language-shell"><code class="language-shell">[root@ java]# vi /etc/profile</code></pre><p>添加如下配置：</p><pre class=" language-shell"><code class="language-shell">export JAVA_HOME=/usr/java/jdk1.8.0_201  export JRE_HOME=${JAVA_HOME}/jre  export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  export PATH=${JAVA_HOME}/bin:$PATH</code></pre><p>执行 <code>source</code> 命令，使得配置立即生效：</p><pre class=" language-shell"><code class="language-shell">[root@ java]# source /etc/profile</code></pre><h3 id="1-3-检查是否安装成功"><a href="#1-3-检查是否安装成功" class="headerlink" title="1.3. 检查是否安装成功"></a><a name="1.3. 检查是否安装成功">1.3. 检查是否安装成功</a></h3><pre class=" language-shell"><code class="language-shell">[root@ java]# java -version</code></pre><p>显示出对应的版本信息则代表安装成功。</p><pre class=" language-shell"><code class="language-shell">java version "1.8.0_201"Java(TM) SE Runtime Environment (build 1.8.0_201-b09)Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)</code></pre><h2 id="二-安装Zookeeper"><a href="#二-安装Zookeeper" class="headerlink" title="二.安装Zookeeper"></a><a name="二.安装Zookeeper">二.安装Zookeeper</a></h2><h3 id="2-1-下载"><a href="#2-1-下载" class="headerlink" title="2.1 下载"></a><a name="2.1 下载">2.1 下载</a></h3><p>下载对应版本 Zookeeper，这里我下载的版本 <code>3.4.14</code>。官方下载地址：<a href="https://archive.apache.org/dist/zookeeper/" target="_blank" rel="external">https://archive.apache.org/dist/zookeeper/</a></p><pre class=" language-shell"><code class="language-shell"># wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz</code></pre><h3 id="2-2-解压"><a href="#2-2-解压" class="headerlink" title="2.2 解压"></a><a name="2.2 解压">2.2 解压</a></h3><pre class=" language-shell"><code class="language-shell"># tar -zxvf zookeeper-3.4.14.tar.gz</code></pre><h3 id="2-3-配置环境变量"><a href="#2-3-配置环境变量" class="headerlink" title="2.3 配置环境变量"></a><a name="2.3 配置环境变量">2.3 配置环境变量</a></h3><pre class=" language-shell"><code class="language-shell"># vim /etc/profile</code></pre><p>添加环境变量：</p><pre class=" language-shell"><code class="language-shell">export ZOOKEEPER_HOME=/usr/app/zookeeper-3.4.14export PATH=$ZOOKEEPER_HOME/bin:$PATH</code></pre><p>使得配置的环境变量生效：</p><pre class=" language-shell"><code class="language-shell"># source /etc/profile</code></pre><h3 id="2-4-修改配置"><a href="#2-4-修改配置" class="headerlink" title="2.4 修改配置"></a><a name="2.4 修改配置">2.4 修改配置</a></h3><p>进入安装目录的 <code>conf/</code> 目录下，拷贝配置样本并进行修改：</p><pre><code># cp zoo_sample.cfg  zoo.cfg</code></pre><p>指定数据存储目录和日志文件目录（目录不用预先创建，程序会自动创建），修改后完整配置如下：</p><pre class=" language-properties"><code class="language-properties"><span class="token comment" spellcheck="true"># The number of milliseconds of each tick</span><span class="token attr-name">tickTime</span><span class="token punctuation">=</span><span class="token attr-value">2000</span><span class="token comment" spellcheck="true"># The number of ticks that the initial</span><span class="token comment" spellcheck="true"># synchronization phase can take</span><span class="token attr-name">initLimit</span><span class="token punctuation">=</span><span class="token attr-value">10</span><span class="token comment" spellcheck="true"># The number of ticks that can pass between</span><span class="token comment" spellcheck="true"># sending a request and getting an acknowledgement</span><span class="token attr-name">syncLimit</span><span class="token punctuation">=</span><span class="token attr-value">5</span><span class="token comment" spellcheck="true"># the directory where the snapshot is stored.</span><span class="token comment" spellcheck="true"># do not use /tmp for storage, /tmp here is just</span><span class="token comment" spellcheck="true"># example sakes.</span><span class="token attr-name">dataDir</span><span class="token punctuation">=</span><span class="token attr-value">/usr/local/zookeeper/data</span><span class="token attr-name">dataLogDir</span><span class="token punctuation">=</span><span class="token attr-value">/usr/local/zookeeper/log</span><span class="token comment" spellcheck="true"># the port at which the clients will connect</span><span class="token attr-name">clientPort</span><span class="token punctuation">=</span><span class="token attr-value">2181</span><span class="token comment" spellcheck="true"># the maximum number of client connections.</span><span class="token comment" spellcheck="true"># increase this if you need to handle more clients</span><span class="token comment" spellcheck="true">#maxClientCnxns=60</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Be sure to read the maintenance section of the</span><span class="token comment" spellcheck="true"># administrator guide before turning on autopurge.</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># The number of snapshots to retain in dataDir</span><span class="token comment" spellcheck="true">#autopurge.snapRetainCount=3</span><span class="token comment" spellcheck="true"># Purge task interval in hours</span><span class="token comment" spellcheck="true"># Set to "0" to disable auto purge feature</span><span class="token comment" spellcheck="true">#autopurge.purgeInterval=1</span></code></pre><blockquote><p>配置参数说明：</p><ul><li><strong>tickTime</strong>：用于计算的基础时间单元。比如 session 超时：N*tickTime；</li><li><strong>initLimit</strong>：用于集群，允许从节点连接并同步到 master 节点的初始化连接时间，以 tickTime 的倍数来表示；</li><li><strong>syncLimit</strong>：用于集群， master 主节点与从节点之间发送消息，请求和应答时间长度（心跳机制）；</li><li><strong>dataDir</strong>：数据存储位置；</li><li><strong>dataLogDir</strong>：日志目录；</li><li><strong>clientPort</strong>：用于客户端连接的端口，默认 2181</li></ul></blockquote><h3 id="2-5-启动"><a href="#2-5-启动" class="headerlink" title="2.5 启动"></a><a name="2.5 启动">2.5 启动</a></h3><p>由于已经配置过环境变量，直接使用下面命令启动即可：</p><pre><code>zkServer.sh start</code></pre><h3 id="2-6-验证是否安装成功"><a href="#2-6-验证是否安装成功" class="headerlink" title="2.6 验证是否安装成功"></a><a name="2.6 验证是否安装成功">2.6 验证是否安装成功</a></h3><p>使用 JPS 验证进程是否已经启动，出现 <code>QuorumPeerMain</code> 则代表启动成功。</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 bin]# jps3814 QuorumPeerMain</code></pre><h2 id="三-安装Hadoop"><a href="#三-安装Hadoop" class="headerlink" title="三.安装Hadoop"></a><a name="三.安装Hadoop">三.安装Hadoop</a></h2><h2 id="3-1-配置免密登录"><a href="#3-1-配置免密登录" class="headerlink" title="3.1 配置免密登录"></a><a name="3.1 配置免密登录">3.1 配置免密登录</a></h2><p>Hadoop 组件之间需要基于 SSH 进行通讯。</p><h3 id="3-1-1-生成公私钥"><a href="#3-1-1-生成公私钥" class="headerlink" title="3.1.1 生成公私钥"></a><a name="3.1.1 生成公私钥">3.1.1 生成公私钥</a></h3><p>执行下面命令行生成公匙和私匙：</p><pre><code>ssh-keygen -t rsa</code></pre><h3 id="3-1-2-授权"><a href="#3-1-2-授权" class="headerlink" title="3.1.2 授权"></a><a name="3.1.2 授权">3.1.2 授权</a></h3><p>进入 <code>~/.ssh</code> 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件：</p><pre class=" language-shell"><code class="language-shell">[root@@hadoop001 sbin]#  cd ~/.ssh[root@@hadoop001 .ssh]# ll-rw-------. 1 root root 1675 3 月  15 09:48 id_rsa-rw-r--r--. 1 root root  388 3 月  15 09:48 id_rsa.pub</code></pre><pre class=" language-shell"><code class="language-shell"># 写入公匙到授权文件[root@hadoop001 .ssh]# cat id_rsa.pub >> authorized_keys[root@hadoop001 .ssh]# chmod 600 authorized_keys</code></pre><h2 id="3-2-Hadoop-HDFS-环境搭建"><a href="#3-2-Hadoop-HDFS-环境搭建" class="headerlink" title="3.2 Hadoop(HDFS)环境搭建"></a><a name="3.2 Hadoop(HDFS)环境搭建">3.2 Hadoop(HDFS)环境搭建</a></h2><h3 id="3-2-1-下载并解压"><a href="#3-2-1-下载并解压" class="headerlink" title="3.2.1 下载并解压"></a><a name="3.2.1 下载并解压">3.2.1 下载并解压</a></h3><p>下载 Hadoop 安装包，这里我下载的是 CDH 版本的，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="external">http://archive.cloudera.com/cdh5/cdh/5/</a></p><pre class=" language-shell"><code class="language-shell"># 解压tar -zvxf hadoop-2.6.0-cdh5.16.2.tar.gz</code></pre><h3 id="3-2-2-配置环境变量"><a href="#3-2-2-配置环境变量" class="headerlink" title="3.2.2 配置环境变量"></a><a name="3.2.2 配置环境变量">3.2.2 配置环境变量</a></h3><pre class=" language-shell"><code class="language-shell"># vi /etc/profile</code></pre><p>配置环境变量：</p><pre><code>export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.16.2export  PATH=${HADOOP_HOME}/bin:$PATH</code></pre><p>执行 <code>source</code> 命令，使得配置的环境变量立即生效：</p><pre class=" language-shell"><code class="language-shell"># source /etc/profile</code></pre><h3 id="3-2-3-修改Hadoop配置"><a href="#3-2-3-修改Hadoop配置" class="headerlink" title="3.2.3 修改Hadoop配置"></a><a name="3.2.3 修改Hadoop配置">3.2.3 修改Hadoop配置</a></h3><p>进入 <code>${HADOOP_HOME}/etc/hadoop/</code> 目录下，修改以下配置：</p><h4 id="1-hadoop-env-sh"><a href="#1-hadoop-env-sh" class="headerlink" title="1. hadoop-env.sh"></a>1. hadoop-env.sh</h4><pre class=" language-shell"><code class="language-shell"># JDK安装路径export  JAVA_HOME=/usr/java/jdk1.8.0_201/</code></pre><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2. core-site.xml"></a>2. core-site.xml</h4><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop001:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!--指定 hadoop 存储临时文件的目录--></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/home/hadoop/tmp<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3. hdfs-site.xml"></a>3. hdfs-site.xml</h4><p>指定副本系数和临时文件存储位置：</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1--></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h4 id="4-slaves"><a href="#4-slaves" class="headerlink" title="4. slaves"></a>4. slaves</h4><p>配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可,编辑slaves设置为：</p><pre class=" language-shell"><code class="language-shell">hadoop001</code></pre><h3 id="3-2-4-关闭防火墙"><a href="#3-2-4-关闭防火墙" class="headerlink" title="3.2.4 关闭防火墙"></a><a name="3.2.4 关闭防火墙">3.2.4 关闭防火墙</a></h3><p>不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面：</p><pre class=" language-shell"><code class="language-shell"># 查看防火墙状态sudo firewall-cmd --state# 关闭防火墙:sudo systemctl stop firewalld.service</code></pre><h3 id="3-2-5-初始化"><a href="#3-2-5-初始化" class="headerlink" title="3.2.5 初始化"></a><a name="3.2.5 初始化">3.2.5 初始化</a></h3><p>第一次启动 Hadoop 时需要进行初始化，进入 <code>${HADOOP_HOME}/bin/</code> 目录下，执行以下命令：</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 bin]# ./hdfs namenode -format</code></pre><h3 id="3-2-6-启动HDFS"><a href="#3-2-6-启动HDFS" class="headerlink" title="3.2.6 启动HDFS"></a><a name="3.2.6 启动HDFS">3.2.6 启动HDFS</a></h3><p>进入 <code>${HADOOP_HOME}/sbin/</code> 目录下，启动 HDFS：</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 sbin]# ./start-dfs.sh</code></pre><h3 id="3-2-7-验证是否启动成功"><a href="#3-2-7-验证是否启动成功" class="headerlink" title="3.2.7 验证是否启动成功"></a><a name="3.2.7 验证是否启动成功">3.2.7 验证是否启动成功</a></h3><p>方式一：执行 <code>jps</code> 查看 <code>NameNode</code> 和 <code>DataNode</code> 服务是否已经启动：</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 hadoop-2.6.0-cdh5.16.2]# jps9137 DataNode9026 NameNode9390 SecondaryNameNode</code></pre><p>方式二：查看 Web UI 界面，端口为 <code>50070</code>：</p><div><img src="http://ww1.sinaimg.cn/large/c4d294bfly1gawcrlnew0j22bg16o48d.jpg"></div><h2 id=""><a href="#" class="headerlink" title=""></a><a name="3.3 Hadoop(YARN)环境搭建"></a></h2><h3 id="3-3-1-修改配置"><a href="#3-3-1-修改配置" class="headerlink" title="3.3.1 修改配置"></a><a name="3.3.1 修改配置">3.3.1 修改配置</a></h3><p>进入 <code>${HADOOP_HOME}/etc/hadoop/</code> 目录下，修改以下配置：</p><h4 id="1-mapred-site-xml"><a href="#1-mapred-site-xml" class="headerlink" title="1. mapred-site.xml"></a>1. mapred-site.xml</h4><pre class=" language-shell"><code class="language-shell"># 如果没有mapred-site.xml，则拷贝一份样例文件后再修改cp mapred-site.xml.template mapred-site.xml</code></pre><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h4 id="2-yarn-site-xml"><a href="#2-yarn-site-xml" class="headerlink" title="2. yarn-site.xml"></a>2. yarn-site.xml</h4><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h3 id="3-3-2-启动服务"><a href="#3-3-2-启动服务" class="headerlink" title="3.3.2 启动服务"></a><a name="3.3.2 启动服务">3.3.2 启动服务</a></h3><p>进入 <code>${HADOOP_HOME}/sbin/</code> 目录下，启动 YARN：</p><pre class=" language-shell"><code class="language-shell">./start-yarn.sh</code></pre><h3 id="3-3-3-验证是否启动成功"><a href="#3-3-3-验证是否启动成功" class="headerlink" title="3.3.3 验证是否启动成功"></a><a name="3.3.3 验证是否启动成功">3.3.3 验证是否启动成功</a></h3><p>方式一：执行 <code>jps</code> 命令查看 <code>NodeManager</code> 和 <code>ResourceManager</code> 服务是否已经启动：</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 hadoop-2.6.0-cdh5.16.2]# jps9137 DataNode9026 NameNode12294 NodeManager12185 ResourceManager9390 SecondaryNameNode</code></pre><p>方式二：查看 Web UI 界面，端口号为 <code>8088</code>：</p><div><img src="http://ww1.sinaimg.cn/large/c4d294bfly1gawcteetpvj22bg16qwr9.jpg"></div><h2 id="四-安装Hbase"><a href="#四-安装Hbase" class="headerlink" title="四.安装Hbase"></a><a name="四.安装Hbase">四.安装Hbase</a></h2><p>HBase 的版本必须要与 Hadoop 的版本兼容，不然会出现各种 Jar 包冲突。这里我 Hadoop 安装的版本为 <code>hadoop-2.6.0-cdh5.16.2</code>，为保持版本一致，选择的 HBase 版本为 <code>hbase-1.2.0-cdh5.14.2</code> 。所有软件版本如下：</p><ul><li>Hadoop 版本： hadoop-2.6.0-cdh5.16.2</li><li>HBase 版本： hbase-1.2.0-cdh5.14.2</li><li>JDK 版本：JDK 1.8</li></ul><p>这里为版本兼容情况</p><div align="center"> <img src="http://ww1.sinaimg.cn/large/c4d294bfly1gawbiwzz7bj20r50n2myr.jpg"></div><h3 id="4-1-软件下载解压"><a href="#4-1-软件下载解压" class="headerlink" title="4.1 软件下载解压"></a><a name="4.1 软件下载解压">4.1 软件下载解压</a></h3><p>下载后进行解压，下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="external">http://archive.cloudera.com/cdh5/cdh/5/</a>    </p><pre class=" language-shell"><code class="language-shell"># tar -zxvf hbase-1.2.0-cdh5.14.2.tar.gz</code></pre><h3 id="4-2-配置环境变量"><a href="#4-2-配置环境变量" class="headerlink" title="4.2 配置环境变量"></a><a name="4.2 配置环境变量">4.2 配置环境变量</a></h3><pre class=" language-shell"><code class="language-shell"># vim /etc/profile</code></pre><p>添加环境变量：</p><pre class=" language-shell"><code class="language-shell">export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.14.2export PATH=$HBASE_HOME/bin:$PATH</code></pre><p>使得配置的环境变量生效：</p><pre class=" language-shell"><code class="language-shell"># source /etc/profile</code></pre><h3 id="4-3-进行HBase相关配置"><a href="#4-3-进行HBase相关配置" class="headerlink" title="4.3 进行HBase相关配置"></a><a name="4.3 进行HBase相关配置">4.3 进行HBase相关配置</a></h3><p>1.修改安装目录下的 <code>conf/hbase-env.sh</code>,指定 JDK 的安装路径：</p><pre class=" language-shell"><code class="language-shell"># The java implementation to use.  Java 1.7+ required.export JAVA_HOME=/usr/java/jdk1.8.0_201# 使用外部zookeeper管理hbaseexport HBASE_MANAGES_ZK=flase</code></pre><p>2.修改安装目录下的 <code>conf/hbase-site.xml</code>，增加如下配置 (hadoop001 为主机名)：</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--指定 HBase 以分布式模式运行--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.cluster.distributed<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!--指定 HBase 数据存储路径为 HDFS 上的 hbase 目录,hbase 目录不需要预先创建，程序会自动创建--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.rootdir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop001:8020/hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--指定 zookeeper 数据的存储位置--></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.zookeeper.property.dataDir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/usr/local/zookeeper/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!--指定 Hbase Web UI 默认端口--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.master.info.port<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>60010<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!--指定外置zookeeper--></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop001:2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>3.修改安装目录下的 <code>conf/regionservers</code>，指定 region  servers 的地址，修改后其内容如下：</p><pre class=" language-shell"><code class="language-shell">hadoop001</code></pre><h3 id="4-4-启动Hbase"><a href="#4-4-启动Hbase" class="headerlink" title="4.4 启动Hbase"></a><a name="4.4 启动Hbase">4.4 启动Hbase</a></h3><pre class=" language-shell"><code class="language-shell"># bin/start-hbase.sh</code></pre><h3 id="4-5-验证是否安装成功"><a href="#4-5-验证是否安装成功" class="headerlink" title="4.5 验证是否安装成功"></a><a name="4.5 验证是否安装成功">4.5 验证是否安装成功</a></h3><p>验证方式一 ：使用 <code>jps</code> 命令查看进程。其中 <code>HMaster</code>，<code>HRegionServer</code> 是 HBase 的进程，<code>HQuorumPeer</code> 是 HBase 内置的 Zookeeper 的进程，其余的为 HDFS 和 YARN 的进程。</p><pre class=" language-shell"><code class="language-shell">[root@hadoop001 conf]# jps28688 NodeManager25824 GradleDaemon10177 Jps22083 HRegionServer20534 DataNode20807 SecondaryNameNode18744 Main20411 NameNode21851 HQuorumPeer28573 ResourceManager21933 HMaster</code></pre><p>验证方式二 ：访问 HBase Web UI 界面，需要注意的是 1.2 版本的 HBase 的访问端口为 <code>60010</code></p><div><img src="http://ww1.sinaimg.cn/large/c4d294bfly1gawcusk23aj22bc170ail.jpg"></div><h2 id="五-安装Phoenix"><a href="#五-安装Phoenix" class="headerlink" title="五.安装Phoenix"></a><a name="五.安装Phoenix">五.安装Phoenix</a></h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p><p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p><h3 id="5-1-下载并解压"><a href="#5-1-下载并解压" class="headerlink" title="5.1 下载并解压"></a><a name="5.1 下载并解压">5.1 下载并解压</a></h3><p>官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: <a href="http://phoenix.apache.org/download.html" target="_blank" rel="external">http://phoenix.apache.org/download.html</a></p><pre class=" language-shell"><code class="language-shell"># 下载wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz# 解压tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</code></pre><h3 id="5-2-拷贝Jar包"><a href="#5-2-拷贝Jar包" class="headerlink" title="5.2 拷贝Jar包"></a><a name="5.2 拷贝Jar包">5.2 拷贝Jar包</a></h3><p>按照官方文档的说明，需要将 <code>phoenix server jar</code> 添加到所有 <code>Region Servers</code> 的安装目录的 <code>lib</code> 目录下。</p><p>这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 <code>Region Servers</code> 机器上。</p><pre class=" language-shell"><code class="language-shell">cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.14.2/lib</code></pre><h3 id="5-3-重启-Region-Servers"><a href="#5-3-重启-Region-Servers" class="headerlink" title="5.3 重启 Region Servers"></a><a name="5.3 重启 Region Servers">5.3 重启 Region Servers</a></h3><pre class=" language-shell"><code class="language-shell"># 停止Hbasestop-hbase.sh# 启动Hbasestart-hbase.sh</code></pre><h3 id="5-4-启动Phoenix"><a href="#5-4-启动Phoenix" class="headerlink" title="5.4 启动Phoenix"></a><a name="5.4 启动Phoenix">5.4 启动Phoenix</a></h3><p>在 Phoenix 解压目录下的 <code>bin</code> 目录下执行如下命令，需要指定 Zookeeper 的地址：</p><ul><li>如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181；</li><li>如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。</li></ul><pre class=" language-shell"><code class="language-shell"># ./sqlline.py hadoop001:2181</code></pre><h3 id="5-5-验证是否安装成功"><a href="#5-5-验证是否安装成功" class="headerlink" title="5.5 验证是否安装成功"></a><a name="5.5 验证是否安装成功">5.5 验证是否安装成功</a></h3><p>启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 <code>!table</code> 或 <code>!tables</code> 查看当前所有表的信息</p><div><img src="http://ww1.sinaimg.cn/large/c4d294bfly1gawcxfmjfjj227m0xsdz6.jpg"></div><p>本文内容部分摘自[<a href="https://github.com/heibaiying/BigData-Notes" target="_blank" rel="external">https://github.com/heibaiying/BigData-Notes</a>]</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot2集成Quartz配置独立数据源</title>
      <link href="/2019/08/05/SpringBoot2%E9%9B%86%E6%88%90Quartz%E9%85%8D%E7%BD%AE%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
      <url>/2019/08/05/SpringBoot2%E9%9B%86%E6%88%90Quartz%E9%85%8D%E7%BD%AE%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<h2 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h2><blockquote><p>Quartz配置需要部署独立的表结构，但是经常存于业务表之间，有些时候可能需要与业务表分开配置，所以在此给Quartz配置独立的数据源</p></blockquote><p>一.版本介绍</p><p>Springboot版本为2.1.6 多数据源配置使用druid进行配置,数据库使用的为Oracle11g,如果使用的是MySQL,直接将数据库的地址和驱动改一下即可</p><pre><code>    &lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;        &lt;relativePath/&gt;    &lt;/parent&gt;    &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt;            &lt;version&gt;1.1.17&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;    &lt;/dependency&gt;</code></pre><p>二.Quartz配置介绍</p><pre><code>spring:  datasource:    type: com.alibaba.druid.pool.DruidDataSource    driver-class-name: oracle.jdbc.driver.OracleDriver    druid:      business: # 配置业务数据源        url: jdbc:oracle:thin:@127.0.0.1:1521:orcl        username: business        password: business      quartz:   #配置Quartz数据源        url: jdbc:oracle:thin:@127.0.0.1:1521:orcl        username: quartz        password: quartz      # 下面为连接池的补充设置，应用到上面所有数据源中      # 初始化大小，最小，最大      initialSize: 5      minIdle: 5      maxActive: 15      # 配置获取连接等待超时的时间      maxWait: 60000      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒      timeBetweenEvictionRunsMillis: 60000      minEvictableIdleTimeMillis: 300000      validationQuery: SELECT 1 FROM DUAL      testWhileIdle: true      testOnBorrow: false      testOnReturn: false      # 打开PSCache，并且指定每个连接上PSCache的大小      poolPreparedStatements: true      # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&#39;wall&#39;用于防火墙      maxPoolPreparedStatementPerConnectionSize: 20      filters: stat,wall,log4j2      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录      connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000      # 合并多个DruidDataSource的监控数据      useGlobalDataSourceStat: true      remove-abandoned: true      remove-abandoned-timeout: 180      log-abandoned: true  quartz:    jdbc:      initialize-schema: never #配置是否每次重启项目都自动生成Quartz表结构,在此使用always生成一次后就可以改为never配置    job-store-type: jdbc    properties:      org:        quartz:          scheduler:            instanceName: etlCleanScheduler            instanceId: AUTO          jobStore:            class: org.quartz.impl.jdbcjobstore.JobStoreTX            driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate            tablePrefix: QRTZ_  #Quartz表前缀            isClustered: true            clusterCheckinInterval: 10000            useProperties: false          threadPool:            class: org.quartz.simpl.SimpleThreadPool            #线程数 一个任务使用一个线程            threadCount: 100            threadPriority: 5            threadsInheritContextClassLoaderOfInitializingThread: true</code></pre><p>三.配置多数据源</p><pre><code>package com.rubikstack.etlclean.config;import com.alibaba.druid.pool.DruidDataSource;import lombok.extern.slf4j.Slf4j;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.SqlSessionFactoryBean;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.autoconfigure.quartz.QuartzDataSource;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import tk.mybatis.spring.annotation.MapperScan;import javax.sql.DataSource;import java.util.Properties;@Slf4j@Configuration@MapperScan(basePackages = DataSourceConfig.MAPPER_PACKAGE, sqlSessionFactoryRef = DataSourceConfig.SESSION_FACTORY)public class DataSourceConfig {    @Value(&quot;${mybatis.queryLimit}&quot;)    private String queryLimit;    static final String SESSION_FACTORY = &quot;dbSqlSessionFactory&quot;;    private static final String DATASOURCE_NAME = &quot;dbDataSource&quot;;    /**     * mapper类的包路径     */    static final String MAPPER_PACKAGE = &quot;com.example.mapper&quot;;    static final String MODEL_PACKAGE = &quot;com.example.model&quot;;    /**     * 自定义mapper的xml文件路径     */    private static final String MAPPER_XML_PATH = &quot;classpath*:com.example.mapper/*Mapper.xml&quot;;    /**     * 数据源配置的前缀，必须与application.properties中配置的对应数据源的前缀一致     */    private static final String BUSINESS_DATASOURCE_PREFIX = &quot;spring.datasource.druid.business&quot;;    private static final String QUARTZ_DATASOURCE_PREFIX = &quot;spring.datasource.druid.quartz&quot;;    @Primary    @Bean(name = DATASOURCE_NAME)    @ConfigurationProperties(prefix = BUSINESS_DATASOURCE_PREFIX)    public DruidDataSource druidDataSource() {        return new DruidDataSource();    }    /**     * 配置Mybatis环境     */    @Primary    @Bean(name = SESSION_FACTORY)    public SqlSessionFactory sqlSessionFactory() {        log.info(&quot;配置SqlSessionFactory开始&quot;);        final SqlSessionFactoryBean sessionFactoryBean = new SqlSessionFactoryBean();        sessionFactoryBean.setDataSource(druidDataSource());        try {            PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();            // 自定义mapper的xml文件地址，当通用mapper提供的默认功能无法满足我们的需求时，可以自己添加实现，与mybatis写mapper一样            sessionFactoryBean.setMapperLocations(resolver.getResources(MAPPER_XML_PATH));            org.apache.ibatis.session.Configuration configuration = new org.apache.ibatis.session.Configuration();            Properties properties = new Properties();            properties.put(&quot;queryLimit&quot;,queryLimit);            configuration.setVariables(properties);            configuration.setMapUnderscoreToCamelCase(true);            configuration.setLogImpl(org.apache.ibatis.logging.stdout.StdOutImpl.class);            sessionFactoryBean.setConfiguration(configuration);            sessionFactoryBean.setTypeAliasesPackage(MODEL_PACKAGE);            return sessionFactoryBean.getObject();        } catch (Exception e) {            log.error(&quot;配置SqlSessionFactory失败，error:{}&quot;, e.getMessage());            throw new RuntimeException(e.getMessage());        }    }    /**     * @QuartzDataSource 注解则是配置Quartz独立数据源的配置     */    @Bean    @QuartzDataSource     @ConfigurationProperties(prefix = QUARTZ_DATASOURCE_PREFIX)    public DataSource quartzDataSource(){        return new DruidDataSource();    }}</code></pre><p>有关@QuartzDataSource的配置请看SpringBoot官方文档<br><a href="https://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#boot-features-quartz" target="_blank" rel="external">@QuartzDataSource</a></p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop基础概念</title>
      <link href="/2019/07/14/Hadoop%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/07/14/Hadoop%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop基础概念"><a href="#Hadoop基础概念" class="headerlink" title="Hadoop基础概念"></a>Hadoop基础概念</h1><h2 id="Hadoop起源"><a href="#Hadoop起源" class="headerlink" title="Hadoop起源"></a>Hadoop起源</h2><pre><code>Hadoop起源于开源网络搜索引擎Apache Nutch，它也是后来的Lucene项目其中的一部分，因为搜索引擎需要大量的存储网页爬取和索引过程中产生的超大文件，所以Nutch团队基于&quot;谷歌分布式文件系统(GFS)&quot;实现了Nutch分布式文件系统(NDFS)，2004年谷歌又推出了MapReduce系统，一种用于大规模数据集(1TB以上)的并行运算系统，于是Nutch的开发团队则又在Nutch的基础上实现了一个MapReduce系统。到了2006年Nutch的开发团队发现这个NDFS+MapReduce不仅仅适用于搜索领域于是则将NDFS和MapReduce移出Nutch形成Lucene的一个子项目，命名为Hadoop。</code></pre><h2 id="Hadoop重要概念"><a href="#Hadoop重要概念" class="headerlink" title="Hadoop重要概念"></a>Hadoop重要概念</h2><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><pre><code>MapReduce的含义是Map(映射)和Reduce(规约)</code></pre><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><pre><code>Map提取输入数据中有效的数据</code></pre><h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><pre><code>Reduce合并过滤多个Map集中提取的有效数据</code></pre><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><pre><code>HDFS是Hadopp自带的分布式文件系统也简称为DFS，分布式文件系统是指当数据集的大小超过一台独立的物理计算机的存储能力时就有必要对它进行分区并存储到若干台单独的计算机上，管理网络中跨多台计算机存储的文件系统称为分布式文件系统。</code></pre><h4 id="HDFS的设计"><a href="#HDFS的设计" class="headerlink" title="HDFS的设计"></a>HDFS的设计</h4><blockquote><p>HDFS以流式数据访问模式来存储超大文件，运行于商用硬件集群上。</p></blockquote><ol><li><p>超大文件:</p><p> 这里指的是几百M，或者几百GB甚至TB大小的文件。</p></li><li><p>流式数据访问</p><p> HDFS的构建思路是一次写入，多次读取，数据集通过从数据源生成或者复制，接着长时间在此数据集上进行各种分析，每次分析都涉及到该数据集的大部分数据甚至全部数据，因此读取整个数据集的时间延迟比第一次写入的时间延时更加重要。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ监控</title>
      <link href="/2019/07/12/RabbitMQ%E7%9B%91%E6%8E%A7/"/>
      <url>/2019/07/12/RabbitMQ%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="RabbitMQ消息队列监控API"><a href="#RabbitMQ消息队列监控API" class="headerlink" title="RabbitMQ消息队列监控API"></a>RabbitMQ消息队列监控API</h1><blockquote><p>请求地址 <a href="http://ip:端口号/接口名称" target="_blank" rel="external">http://ip:端口号/接口名称</a></p></blockquote><pre><code>建议监控数据收集间隔为60秒一次</code></pre><table><thead><tr><th>HTTP API URL</th><th>HTTP请求类型</th><th>接口含义</th></tr></thead><tbody><tr><td>/api/connections</td><td>GET</td><td>获取当前RabbitMQ集群下所有打开的连接</td></tr><tr><td>/api/nodes</td><td>GET</td><td>获取当前RabbitMQ集群下所有节点实例的状态信息</td></tr><tr><td>/api/vhosts/{vhost}/connections</td><td>GET</td><td>获取某一个虚拟机主机下的所有打开的connection连接</td></tr><tr><td>/api/connections/{name}/channels</td><td>GET</td><td>获取某一个连接下所有的管道信息</td></tr><tr><td>/api/vhosts/{vhost}/channels</td><td>GET</td><td>获取某一个虚拟机主机下的管道信息</td></tr><tr><td>/api/consumers/{vhost}</td><td>GET</td><td>获取某一个虚拟机主机下的所有消费者信息</td></tr><tr><td>/api/exchanges/{vhost}</td><td>GET</td><td>获取某一个虚拟机主机下面的所有交换器信息</td></tr><tr><td>/api/queues/{vhost}</td><td>GET</td><td>获取某一个虚拟机主机下的所有队列信息</td></tr><tr><td>/api/users</td><td>GET</td><td>获取集群中所有的用户信息</td></tr><tr><td>/api/users/{name}</td><td>GET/PUT/DELETE</td><td>获取/更新/删除指定用户信息</td></tr><tr><td>/api/users/{user}/permissions</td><td>GET</td><td>获取当前指定用户的所有权限信息</td></tr><tr><td>/api/permissions/{vhost}/{user}</td><td>GET/PUT/DELETE</td><td>获取/更新/删除指定虚拟主机下特定用户的权限</td></tr><tr><td>/api/exchanges/{vhost}/{name}/publish</td><td>POST</td><td>在指定的虚拟机主机和交换器上发布一个消息</td></tr><tr><td>/api/queues/{vhost}/{name}/get</td><td>POST</td><td>测在指定虚拟机主机和队列名中获取消息，同时该动作会修改队列状态试</td></tr><tr><td>/api/healthchecks/node/{node}</td><td>GET</td><td>获取指定节点的健康检查状态</td></tr><tr><td>/api/nodes/{node}/memory</td><td>GET</td><td>获取指定节点的内存使用信息</td></tr></tbody></table><h1 id="部分API实例"><a href="#部分API实例" class="headerlink" title="部分API实例"></a>部分API实例</h1><h2 id="api-overview-返回集群度量标准"><a href="#api-overview-返回集群度量标准" class="headerlink" title="/api/overview 返回集群度量标准"></a>/api/overview 返回集群度量标准</h2><table><thead><tr><th>字段名称</th><th>意义</th></tr></thead><tbody><tr><td>cluster_name</td><td>集群名称</td></tr><tr><td>management_version</td><td>版本</td></tr><tr><td>message_stats</td><td>消息速率</td></tr><tr><td>object_totals.connections</td><td>连接总数</td></tr><tr><td>object_totals.channels</td><td>频道总数</td></tr><tr><td>object_totals.queues</td><td>队列总数</td></tr><tr><td>object_totals.consumers</td><td>消费者总数</td></tr><tr><td>queue_totals.messages</td><td>准备好与未确认的消息总数</td></tr><tr><td>queue_totals.messages_ready</td><td>准备发送的消息数</td></tr><tr><td>queue_totals.messages_unacknowledged</td><td>未确认消息的数量</td></tr><tr><td>message_stats.publish`</td><td>最近发布的消息</td></tr><tr><td>message_stats.publish_details.rate</td><td>消息发布率</td></tr><tr><td>message_stats.deliver_get</td><td>最近发送给消费者的消息</td></tr><tr><td>message_stats.deliver_get.rate</td><td>邮件传递率</td></tr><tr><td>message_stats</td><td>其他消息统计</td></tr></tbody></table><pre><code>    {        &quot;management_version&quot;: &quot;3.7.14&quot;,        &quot;rates_mode&quot;: &quot;basic&quot;,        &quot;sample_retention_policies&quot;: {            &quot;global&quot;: [                600,                3600,                28800,                86400            ],            &quot;basic&quot;: [                600,                3600            ],            &quot;detailed&quot;: [                600            ]        },        &quot;exchange_types&quot;: [            {                &quot;name&quot;: &quot;fanout&quot;,                &quot;description&quot;: &quot;AMQP fanout exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;headers&quot;,                &quot;description&quot;: &quot;AMQP headers exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;x-delayed-message&quot;,                &quot;description&quot;: &quot;Delayed Message Exchange.&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;direct&quot;,                &quot;description&quot;: &quot;AMQP direct exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;topic&quot;,                &quot;description&quot;: &quot;AMQP topic exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            }        ],        &quot;rabbitmq_version&quot;: &quot;3.7.14&quot;,        &quot;cluster_name&quot;: &quot;rabbit@wangtaodeMacBook-Pro&quot;,        &quot;erlang_version&quot;: &quot;21.3.2&quot;,        &quot;erlang_full_version&quot;: &quot;Erlang/OTP 21 [erts-10.3.1] [source]        [64-bit] [smp:12:12] [ds:12:12:10] [async-threads:192]         [hipe] [dtrace]&quot;,        &quot;message_stats&quot;: {            &quot;disk_reads&quot;: 2,            &quot;disk_reads_details&quot;: {                &quot;rate&quot;: 0            },            &quot;disk_writes&quot;: 0,            &quot;disk_writes_details&quot;: {                &quot;rate&quot;: 0            }        },        &quot;churn_rates&quot;: {            &quot;channel_closed&quot;: 0,            &quot;channel_closed_details&quot;: {                &quot;rate&quot;: 0            },            &quot;channel_created&quot;: 0,            &quot;channel_created_details&quot;: {                &quot;rate&quot;: 0            },            &quot;connection_closed&quot;: 0,            &quot;connection_closed_details&quot;: {                &quot;rate&quot;: 0            },            &quot;connection_created&quot;: 0,            &quot;connection_created_details&quot;: {                &quot;rate&quot;: 0            },            &quot;queue_created&quot;: 0,            &quot;queue_created_details&quot;: {                &quot;rate&quot;: 0            },            &quot;queue_declared&quot;: 0,            &quot;queue_declared_details&quot;: {                &quot;rate&quot;: 0            },            &quot;queue_deleted&quot;: 0,            &quot;queue_deleted_details&quot;: {                &quot;rate&quot;: 0            }        },        &quot;queue_totals&quot;: {            &quot;messages&quot;: 2,            &quot;messages_details&quot;: {                &quot;rate&quot;: 0            },            &quot;messages_ready&quot;: 2,            &quot;messages_ready_details&quot;: {                &quot;rate&quot;: 0            },            &quot;messages_unacknowledged&quot;: 0,            &quot;messages_unacknowledged_details&quot;: {                &quot;rate&quot;: 0            }        },        &quot;object_totals&quot;: {            &quot;channels&quot;: 0,            &quot;connections&quot;: 0,            &quot;consumers&quot;: 0,            &quot;exchanges&quot;: 16,            &quot;queues&quot;: 10        },        &quot;statistics_db_event_queue&quot;: 0,        &quot;node&quot;: &quot;rabbit@localhost&quot;,        &quot;listeners&quot;: [            {                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;protocol&quot;: &quot;amqp&quot;,                &quot;ip_address&quot;: &quot;127.0.0.1&quot;,                &quot;port&quot;: 5672,                &quot;socket_opts&quot;: {                    &quot;backlog&quot;: 128,                    &quot;nodelay&quot;: true,                    &quot;linger&quot;: [                        true,                        0                    ],                    &quot;exit_on_close&quot;: false                }            },            {                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;protocol&quot;: &quot;clustering&quot;,                &quot;ip_address&quot;: &quot;::&quot;,                &quot;port&quot;: 25672,                &quot;socket_opts&quot;: []            },            {                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;protocol&quot;: &quot;http&quot;,                &quot;ip_address&quot;: &quot;::&quot;,                &quot;port&quot;: 15672,                &quot;socket_opts&quot;: {                    &quot;port&quot;: 15672                }            },            {                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;protocol&quot;: &quot;mqtt&quot;,                &quot;ip_address&quot;: &quot;::&quot;,                &quot;port&quot;: 1883,                &quot;socket_opts&quot;: {                    &quot;backlog&quot;: 128,                    &quot;nodelay&quot;: true                }            },            {                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;protocol&quot;: &quot;stomp&quot;,                &quot;ip_address&quot;: &quot;::&quot;,                &quot;port&quot;: 61613,                &quot;socket_opts&quot;: {                    &quot;backlog&quot;: 128,                    &quot;nodelay&quot;: true                }            }        ],        &quot;contexts&quot;: [            {                &quot;ssl_opts&quot;: [],                &quot;node&quot;: &quot;rabbit@localhost&quot;,                &quot;description&quot;: &quot;RabbitMQ Management&quot;,                &quot;path&quot;: &quot;/&quot;,                &quot;port&quot;: &quot;15672&quot;            }        ]    }</code></pre><h2 id="api-nodes-返回所有集群成员统计消息"><a href="#api-nodes-返回所有集群成员统计消息" class="headerlink" title="/api/nodes 返回所有集群成员统计消息"></a>/api/nodes 返回所有集群成员统计消息</h2><table><thead><tr><th>字段名称</th><th>意义</th></tr></thead><tbody><tr><td>mem_used</td><td>使用的内存总量</td></tr><tr><td>mem_limit</td><td>内存使用率</td></tr><tr><td>mem_alarm</td><td>内存警报开关</td></tr><tr><td>disk_free_limit</td><td>磁盘空间限制大小</td></tr><tr><td>disk_free_alarm</td><td>磁盘空间大小限制开关</td></tr><tr><td>fd_total</td><td>文件描述符限制</td></tr><tr><td>fd_used</td><td>文件描述符是否开始</td></tr><tr><td>io_file_handle_open_attempt_count</td><td>io文件句柄打开尝试计数</td></tr><tr><td>sockets_total</td><td>socket数量</td></tr><tr><td>sockets_used</td><td>正在使用的socket数量</td></tr><tr><td>message_stats.disk_reads</td><td>磁盘读取的消息数</td></tr><tr><td>message_stats.disk_writes</td><td>写入磁盘的消息数</td></tr><tr><td>cluster_links</td><td>节点间通信链路</td></tr><tr><td>gc_num</td><td>GC次数</td></tr><tr><td>gc_bytes_reclaimed</td><td>GC回收的字节数</td></tr><tr><td>proc_total</td><td>Erlang进程限制数量</td></tr><tr><td>proc_used</td><td>Erlang进行使用数量</td></tr><tr><td>run_queue</td><td>运行时队列数量</td></tr></tbody></table><pre><code>[    {        &quot;partitions&quot;: [],        &quot;os_pid&quot;: &quot;4470&quot;,        &quot;fd_total&quot;: 256,        &quot;sockets_total&quot;: 138,        &quot;mem_limit&quot;: 6871947673,        &quot;mem_alarm&quot;: false,        &quot;disk_free_limit&quot;: 50000000,        &quot;disk_free_alarm&quot;: false,        &quot;proc_total&quot;: 1048576,        &quot;rates_mode&quot;: &quot;basic&quot;,        &quot;uptime&quot;: 4176419,        &quot;run_queue&quot;: 1,        &quot;processors&quot;: 12,        &quot;exchange_types&quot;: [            {                &quot;name&quot;: &quot;fanout&quot;,                &quot;description&quot;: &quot;AMQP fanout exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;headers&quot;,                &quot;description&quot;: &quot;AMQP headers exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;x-delayed-message&quot;,                &quot;description&quot;: &quot;Delayed Message Exchange.&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;direct&quot;,                &quot;description&quot;: &quot;AMQP direct exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;topic&quot;,                &quot;description&quot;: &quot;AMQP topic exchange, as per the AMQP specification&quot;,                &quot;enabled&quot;: true            }        ],        &quot;auth_mechanisms&quot;: [            {                &quot;name&quot;: &quot;RABBIT-CR-DEMO&quot;,                &quot;description&quot;: &quot;RabbitMQ Demo challenge-response authentication mechanism&quot;,                &quot;enabled&quot;: false            },            {                &quot;name&quot;: &quot;AMQPLAIN&quot;,                &quot;description&quot;: &quot;QPid AMQPLAIN mechanism&quot;,                &quot;enabled&quot;: true            },            {                &quot;name&quot;: &quot;PLAIN&quot;,                &quot;description&quot;: &quot;SASL PLAIN authentication mechanism&quot;,                &quot;enabled&quot;: true            }        ],        &quot;applications&quot;: [            {                &quot;name&quot;: &quot;amqp10_common&quot;,                &quot;description&quot;: &quot;Modules shared by rabbitmq-amqp1.0                 and rabbitmq-amqp1.0-client&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;amqp_client&quot;,                &quot;description&quot;: &quot;RabbitMQ AMQP Client&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;asn1&quot;,                &quot;description&quot;: &quot;The Erlang ASN1 compiler version 5.0.8&quot;,                &quot;version&quot;: &quot;5.0.8&quot;            },            {                &quot;name&quot;: &quot;compiler&quot;,                &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,                &quot;version&quot;: &quot;7.3.2&quot;            },            {                &quot;name&quot;: &quot;cowboy&quot;,                &quot;description&quot;: &quot;Small, fast, modern HTTP server.&quot;,                &quot;version&quot;: &quot;2.6.1&quot;            },            {                &quot;name&quot;: &quot;cowlib&quot;,                &quot;description&quot;: &quot;Support library for manipulating Web protocols.&quot;,                &quot;version&quot;: &quot;2.7.0&quot;            },            {                &quot;name&quot;: &quot;crypto&quot;,                &quot;description&quot;: &quot;CRYPTO&quot;,                &quot;version&quot;: &quot;4.4.1&quot;            },            {                &quot;name&quot;: &quot;goldrush&quot;,                &quot;description&quot;: &quot;Erlang event stream processor&quot;,                &quot;version&quot;: &quot;0.1.9&quot;            },            {                &quot;name&quot;: &quot;inets&quot;,                &quot;description&quot;: &quot;INETS  CXC 138 49&quot;,                &quot;version&quot;: &quot;7.0.6&quot;            },            {                &quot;name&quot;: &quot;jsx&quot;,                &quot;description&quot;: &quot;a streaming, evented json parsing toolkit&quot;,                &quot;version&quot;: &quot;2.9.0&quot;            },            {                &quot;name&quot;: &quot;kernel&quot;,                &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,                &quot;version&quot;: &quot;6.3&quot;            },            {                &quot;name&quot;: &quot;lager&quot;,                &quot;description&quot;: &quot;Erlang logging framework&quot;,                &quot;version&quot;: &quot;3.6.9&quot;            },            {                &quot;name&quot;: &quot;mnesia&quot;,                &quot;description&quot;: &quot;MNESIA  CXC 138 12&quot;,                &quot;version&quot;: &quot;4.15.6&quot;            },            {                &quot;name&quot;: &quot;os_mon&quot;,                &quot;description&quot;: &quot;CPO  CXC 138 46&quot;,                &quot;version&quot;: &quot;2.4.7&quot;            },            {                &quot;name&quot;: &quot;public_key&quot;,                &quot;description&quot;: &quot;Public key infrastructure&quot;,                &quot;version&quot;: &quot;1.6.5&quot;            },            {                &quot;name&quot;: &quot;rabbit&quot;,                &quot;description&quot;: &quot;RabbitMQ&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbit_common&quot;,                &quot;description&quot;: &quot;Modules shared by rabbitmq-server                 and rabbitmq-erlang-client&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_amqp1_0&quot;,                &quot;description&quot;: &quot;AMQP 1.0 support for RabbitMQ&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_delayed_message_exchange&quot;,                &quot;description&quot;: &quot;RabbitMQ Delayed Message Exchange&quot;,                &quot;version&quot;: &quot;20171201-3.7.x&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_management&quot;,                &quot;description&quot;: &quot;RabbitMQ Management Console&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_management_agent&quot;,                &quot;description&quot;: &quot;RabbitMQ Management Agent&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_mqtt&quot;,                &quot;description&quot;: &quot;RabbitMQ MQTT Adapter&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_stomp&quot;,                &quot;description&quot;: &quot;RabbitMQ STOMP plugin&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;rabbitmq_web_dispatch&quot;,                &quot;description&quot;: &quot;RabbitMQ Web Dispatcher&quot;,                &quot;version&quot;: &quot;3.7.14&quot;            },            {                &quot;name&quot;: &quot;ranch&quot;,                &quot;description&quot;: &quot;Socket acceptor pool for TCP protocols.&quot;,                &quot;version&quot;: &quot;1.7.1&quot;            },            {                &quot;name&quot;: &quot;recon&quot;,                &quot;description&quot;: &quot;Diagnostic tools for production use&quot;,                &quot;version&quot;: &quot;2.4.0&quot;            },            {                &quot;name&quot;: &quot;sasl&quot;,                &quot;description&quot;: &quot;SASL  CXC 138 11&quot;,                &quot;version&quot;: &quot;3.3&quot;            },            {                &quot;name&quot;: &quot;ssl&quot;,                &quot;description&quot;: &quot;Erlang/OTP SSL application&quot;,                &quot;version&quot;: &quot;9.2.1&quot;            },            {                &quot;name&quot;: &quot;stdlib&quot;,                &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,                &quot;version&quot;: &quot;3.8&quot;            },            {                &quot;name&quot;: &quot;syntax_tools&quot;,                &quot;description&quot;: &quot;Syntax tools&quot;,                &quot;version&quot;: &quot;2.1.7&quot;            },            {                &quot;name&quot;: &quot;sysmon_handler&quot;,                &quot;description&quot;: &quot;Rate-limiting system_monitor event handler&quot;,                &quot;version&quot;: &quot;1.1.0&quot;            },            {                &quot;name&quot;: &quot;xmerl&quot;,                &quot;description&quot;: &quot;XML parser&quot;,                &quot;version&quot;: &quot;1.3.20&quot;            }        ],        &quot;contexts&quot;: [            {                &quot;description&quot;: &quot;RabbitMQ Management&quot;,                &quot;path&quot;: &quot;/&quot;,                &quot;port&quot;: &quot;15672&quot;            }        ],        &quot;log_files&quot;: [            &quot;/usr/local/var/log/rabbitmq/rabbit@localhost.log&quot;,            &quot;/usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log&quot;        ],        &quot;db_dir&quot;: &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost&quot;,        &quot;config_files&quot;: [],        &quot;net_ticktime&quot;: 60,        &quot;enabled_plugins&quot;: [            &quot;rabbitmq_amqp1_0&quot;,            &quot;rabbitmq_delayed_message_exchange&quot;,            &quot;rabbitmq_management&quot;,            &quot;rabbitmq_mqtt&quot;,            &quot;rabbitmq_stomp&quot;        ],        &quot;mem_calculation_strategy&quot;: &quot;rss&quot;,        &quot;name&quot;: &quot;rabbit@localhost&quot;,        &quot;type&quot;: &quot;disc&quot;,        &quot;running&quot;: true,        &quot;mem_used&quot;: 86556672,        &quot;mem_used_details&quot;: {            &quot;rate&quot;: 4096        },        &quot;fd_used&quot;: 72,        &quot;fd_used_details&quot;: {            &quot;rate&quot;: -0.4        },        &quot;sockets_used&quot;: 0,        &quot;sockets_used_details&quot;: {            &quot;rate&quot;: 0        },        &quot;proc_used&quot;: 457,        &quot;proc_used_details&quot;: {            &quot;rate&quot;: 0        },        &quot;disk_free&quot;: 172579655680,        &quot;disk_free_details&quot;: {            &quot;rate&quot;: -15564.8        },        &quot;gc_num&quot;: 22686,        &quot;gc_num_details&quot;: {            &quot;rate&quot;: 4.6        },        &quot;gc_bytes_reclaimed&quot;: 1060411720,        &quot;gc_bytes_reclaimed_details&quot;: {            &quot;rate&quot;: 158604.8        },        &quot;context_switches&quot;: 307064,        &quot;context_switches_details&quot;: {            &quot;rate&quot;: 37.4        },        &quot;io_read_count&quot;: 22,        &quot;io_read_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_read_bytes&quot;: 2083,        &quot;io_read_bytes_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_read_avg_time&quot;: 0.02909090909090909,        &quot;io_read_avg_time_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_write_count&quot;: 0,        &quot;io_write_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_write_bytes&quot;: 0,        &quot;io_write_bytes_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_write_avg_time&quot;: 0,        &quot;io_write_avg_time_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_sync_count&quot;: 0,        &quot;io_sync_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_sync_avg_time&quot;: 0,        &quot;io_sync_avg_time_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_seek_count&quot;: 0,        &quot;io_seek_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_seek_avg_time&quot;: 0,        &quot;io_seek_avg_time_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_reopen_count&quot;: 0,        &quot;io_reopen_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;mnesia_ram_tx_count&quot;: 60,        &quot;mnesia_ram_tx_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;mnesia_disk_tx_count&quot;: 10,        &quot;mnesia_disk_tx_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;msg_store_read_count&quot;: 0,        &quot;msg_store_read_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;msg_store_write_count&quot;: 0,        &quot;msg_store_write_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_index_journal_write_count&quot;: 0,        &quot;queue_index_journal_write_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_index_write_count&quot;: 0,        &quot;queue_index_write_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_index_read_count&quot;: 3,        &quot;queue_index_read_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_file_handle_open_attempt_count&quot;: 73,        &quot;io_file_handle_open_attempt_count_details&quot;: {            &quot;rate&quot;: 0        },        &quot;io_file_handle_open_attempt_avg_time&quot;: 0.031643835616438354,        &quot;io_file_handle_open_attempt_avg_time_details&quot;: {            &quot;rate&quot;: 0        },        &quot;connection_created&quot;: 0,        &quot;connection_created_details&quot;: {            &quot;rate&quot;: 0        },        &quot;connection_closed&quot;: 0,        &quot;connection_closed_details&quot;: {            &quot;rate&quot;: 0        },        &quot;channel_created&quot;: 0,        &quot;channel_created_details&quot;: {            &quot;rate&quot;: 0        },        &quot;channel_closed&quot;: 0,        &quot;channel_closed_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_declared&quot;: 0,        &quot;queue_declared_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_created&quot;: 0,        &quot;queue_created_details&quot;: {            &quot;rate&quot;: 0        },        &quot;queue_deleted&quot;: 0,        &quot;queue_deleted_details&quot;: {            &quot;rate&quot;: 0        },        &quot;cluster_links&quot;: [],        &quot;metrics_gc_queue_length&quot;: {            &quot;connection_closed&quot;: 0,            &quot;channel_closed&quot;: 0,            &quot;consumer_deleted&quot;: 0,            &quot;exchange_deleted&quot;: 0,            &quot;queue_deleted&quot;: 0,            &quot;vhost_deleted&quot;: 0,            &quot;node_node_deleted&quot;: 0,            &quot;channel_consumer_deleted&quot;: 0        }    }]</code></pre><h2 id="api-nodes-node名称-返回单个节点的统计信息"><a href="#api-nodes-node名称-返回单个节点的统计信息" class="headerlink" title="/api/nodes/{node名称} 返回单个节点的统计信息"></a>/api/nodes/{node名称} 返回单个节点的统计信息</h2><pre><code>{    &quot;partitions&quot;: [],    &quot;os_pid&quot;: &quot;4470&quot;,    &quot;fd_total&quot;: 256,    &quot;sockets_total&quot;: 138,    &quot;mem_limit&quot;: 6871947673,    &quot;mem_alarm&quot;: false,    &quot;disk_free_limit&quot;: 50000000,    &quot;disk_free_alarm&quot;: false,    &quot;proc_total&quot;: 1048576,    &quot;rates_mode&quot;: &quot;basic&quot;,    &quot;uptime&quot;: 11728280,    &quot;run_queue&quot;: 1,    &quot;processors&quot;: 12,    &quot;exchange_types&quot;: [        {            &quot;name&quot;: &quot;fanout&quot;,            &quot;description&quot;: &quot;AMQP fanout exchange, as per the AMQP specification&quot;,            &quot;enabled&quot;: true        },        {            &quot;name&quot;: &quot;headers&quot;,            &quot;description&quot;: &quot;AMQP headers exchange, as per the AMQP specification&quot;,            &quot;enabled&quot;: true        },        {            &quot;name&quot;: &quot;x-delayed-message&quot;,            &quot;description&quot;: &quot;Delayed Message Exchange.&quot;,            &quot;enabled&quot;: true        },        {            &quot;name&quot;: &quot;direct&quot;,            &quot;description&quot;: &quot;AMQP direct exchange, as per the AMQP specification&quot;,            &quot;enabled&quot;: true        },        {            &quot;name&quot;: &quot;topic&quot;,            &quot;description&quot;: &quot;AMQP topic exchange, as per the AMQP specification&quot;,            &quot;enabled&quot;: true        }    ],    &quot;auth_mechanisms&quot;: [        {            &quot;name&quot;: &quot;RABBIT-CR-DEMO&quot;,            &quot;description&quot;: &quot;RabbitMQ Demo challenge-response authentication mechanism&quot;,            &quot;enabled&quot;: false        },        {            &quot;name&quot;: &quot;AMQPLAIN&quot;,            &quot;description&quot;: &quot;QPid AMQPLAIN mechanism&quot;,            &quot;enabled&quot;: true        },        {            &quot;name&quot;: &quot;PLAIN&quot;,            &quot;description&quot;: &quot;SASL PLAIN authentication mechanism&quot;,            &quot;enabled&quot;: true        }    ],    &quot;applications&quot;: [        {            &quot;name&quot;: &quot;amqp10_common&quot;,            &quot;description&quot;: &quot;Modules shared by rabbitmq-amqp1.0 and rabbitmq-amqp1.0-client&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;amqp_client&quot;,            &quot;description&quot;: &quot;RabbitMQ AMQP Client&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;asn1&quot;,            &quot;description&quot;: &quot;The Erlang ASN1 compiler version 5.0.8&quot;,            &quot;version&quot;: &quot;5.0.8&quot;        },        {            &quot;name&quot;: &quot;compiler&quot;,            &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,            &quot;version&quot;: &quot;7.3.2&quot;        },        {            &quot;name&quot;: &quot;cowboy&quot;,            &quot;description&quot;: &quot;Small, fast, modern HTTP server.&quot;,            &quot;version&quot;: &quot;2.6.1&quot;        },        {            &quot;name&quot;: &quot;cowlib&quot;,            &quot;description&quot;: &quot;Support library for manipulating Web protocols.&quot;,            &quot;version&quot;: &quot;2.7.0&quot;        },        {            &quot;name&quot;: &quot;crypto&quot;,            &quot;description&quot;: &quot;CRYPTO&quot;,            &quot;version&quot;: &quot;4.4.1&quot;        },        {            &quot;name&quot;: &quot;goldrush&quot;,            &quot;description&quot;: &quot;Erlang event stream processor&quot;,            &quot;version&quot;: &quot;0.1.9&quot;        },        {            &quot;name&quot;: &quot;inets&quot;,            &quot;description&quot;: &quot;INETS  CXC 138 49&quot;,            &quot;version&quot;: &quot;7.0.6&quot;        },        {            &quot;name&quot;: &quot;jsx&quot;,            &quot;description&quot;: &quot;a streaming, evented json parsing toolkit&quot;,            &quot;version&quot;: &quot;2.9.0&quot;        },        {            &quot;name&quot;: &quot;kernel&quot;,            &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,            &quot;version&quot;: &quot;6.3&quot;        },        {            &quot;name&quot;: &quot;lager&quot;,            &quot;description&quot;: &quot;Erlang logging framework&quot;,            &quot;version&quot;: &quot;3.6.9&quot;        },        {            &quot;name&quot;: &quot;mnesia&quot;,            &quot;description&quot;: &quot;MNESIA  CXC 138 12&quot;,            &quot;version&quot;: &quot;4.15.6&quot;        },        {            &quot;name&quot;: &quot;os_mon&quot;,            &quot;description&quot;: &quot;CPO  CXC 138 46&quot;,            &quot;version&quot;: &quot;2.4.7&quot;        },        {            &quot;name&quot;: &quot;public_key&quot;,            &quot;description&quot;: &quot;Public key infrastructure&quot;,            &quot;version&quot;: &quot;1.6.5&quot;        },        {            &quot;name&quot;: &quot;rabbit&quot;,            &quot;description&quot;: &quot;RabbitMQ&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbit_common&quot;,            &quot;description&quot;: &quot;Modules shared by rabbitmq-server and rabbitmq-erlang-client&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_amqp1_0&quot;,            &quot;description&quot;: &quot;AMQP 1.0 support for RabbitMQ&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_delayed_message_exchange&quot;,            &quot;description&quot;: &quot;RabbitMQ Delayed Message Exchange&quot;,            &quot;version&quot;: &quot;20171201-3.7.x&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_management&quot;,            &quot;description&quot;: &quot;RabbitMQ Management Console&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_management_agent&quot;,            &quot;description&quot;: &quot;RabbitMQ Management Agent&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_mqtt&quot;,            &quot;description&quot;: &quot;RabbitMQ MQTT Adapter&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_stomp&quot;,            &quot;description&quot;: &quot;RabbitMQ STOMP plugin&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;rabbitmq_web_dispatch&quot;,            &quot;description&quot;: &quot;RabbitMQ Web Dispatcher&quot;,            &quot;version&quot;: &quot;3.7.14&quot;        },        {            &quot;name&quot;: &quot;ranch&quot;,            &quot;description&quot;: &quot;Socket acceptor pool for TCP protocols.&quot;,            &quot;version&quot;: &quot;1.7.1&quot;        },        {            &quot;name&quot;: &quot;recon&quot;,            &quot;description&quot;: &quot;Diagnostic tools for production use&quot;,            &quot;version&quot;: &quot;2.4.0&quot;        },        {            &quot;name&quot;: &quot;sasl&quot;,            &quot;description&quot;: &quot;SASL  CXC 138 11&quot;,            &quot;version&quot;: &quot;3.3&quot;        },        {            &quot;name&quot;: &quot;ssl&quot;,            &quot;description&quot;: &quot;Erlang/OTP SSL application&quot;,            &quot;version&quot;: &quot;9.2.1&quot;        },        {            &quot;name&quot;: &quot;stdlib&quot;,            &quot;description&quot;: &quot;ERTS  CXC 138 10&quot;,            &quot;version&quot;: &quot;3.8&quot;        },        {            &quot;name&quot;: &quot;syntax_tools&quot;,            &quot;description&quot;: &quot;Syntax tools&quot;,            &quot;version&quot;: &quot;2.1.7&quot;        },        {            &quot;name&quot;: &quot;sysmon_handler&quot;,            &quot;description&quot;: &quot;Rate-limiting system_monitor event handler&quot;,            &quot;version&quot;: &quot;1.1.0&quot;        },        {            &quot;name&quot;: &quot;xmerl&quot;,            &quot;description&quot;: &quot;XML parser&quot;,            &quot;version&quot;: &quot;1.3.20&quot;        }    ],    &quot;contexts&quot;: [        {            &quot;description&quot;: &quot;RabbitMQ Management&quot;,            &quot;path&quot;: &quot;/&quot;,            &quot;port&quot;: &quot;15672&quot;        }    ],    &quot;log_files&quot;: [        &quot;/usr/local/var/log/rabbitmq/rabbit@localhost.log&quot;,        &quot;/usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log&quot;    ],    &quot;db_dir&quot;: &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost&quot;,    &quot;config_files&quot;: [],    &quot;net_ticktime&quot;: 60,    &quot;enabled_plugins&quot;: [        &quot;rabbitmq_amqp1_0&quot;,        &quot;rabbitmq_delayed_message_exchange&quot;,        &quot;rabbitmq_management&quot;,        &quot;rabbitmq_mqtt&quot;,        &quot;rabbitmq_stomp&quot;    ],    &quot;mem_calculation_strategy&quot;: &quot;rss&quot;,    &quot;name&quot;: &quot;rabbit@localhost&quot;,    &quot;running&quot;: true,    &quot;type&quot;: &quot;disc&quot;,    &quot;mem_used&quot;: 86814720,    &quot;mem_used_details&quot;: {        &quot;rate&quot;: -117964.8    },    &quot;fd_used&quot;: 72,    &quot;fd_used_details&quot;: {        &quot;rate&quot;: 0    },    &quot;sockets_used&quot;: 0,    &quot;sockets_used_details&quot;: {        &quot;rate&quot;: 0    },    &quot;proc_used&quot;: 457,    &quot;proc_used_details&quot;: {        &quot;rate&quot;: 0    },    &quot;disk_free&quot;: 172549808128,    &quot;disk_free_details&quot;: {        &quot;rate&quot;: -31129.6    },    &quot;gc_num&quot;: 38308,    &quot;gc_num_details&quot;: {        &quot;rate&quot;: 3.4    },    &quot;gc_bytes_reclaimed&quot;: 1616503320,    &quot;gc_bytes_reclaimed_details&quot;: {        &quot;rate&quot;: 152201.6    },    &quot;context_switches&quot;: 423307,    &quot;context_switches_details&quot;: {        &quot;rate&quot;: 37.6    },    &quot;io_read_count&quot;: 22,    &quot;io_read_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_read_bytes&quot;: 2083,    &quot;io_read_bytes_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_read_avg_time&quot;: 0.02909090909090909,    &quot;io_read_avg_time_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_write_count&quot;: 0,    &quot;io_write_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_write_bytes&quot;: 0,    &quot;io_write_bytes_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_write_avg_time&quot;: 0,    &quot;io_write_avg_time_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_sync_count&quot;: 0,    &quot;io_sync_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_sync_avg_time&quot;: 0,    &quot;io_sync_avg_time_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_seek_count&quot;: 0,    &quot;io_seek_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_seek_avg_time&quot;: 0,    &quot;io_seek_avg_time_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_reopen_count&quot;: 0,    &quot;io_reopen_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;mnesia_ram_tx_count&quot;: 60,    &quot;mnesia_ram_tx_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;mnesia_disk_tx_count&quot;: 10,    &quot;mnesia_disk_tx_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;msg_store_read_count&quot;: 0,    &quot;msg_store_read_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;msg_store_write_count&quot;: 0,    &quot;msg_store_write_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_index_journal_write_count&quot;: 0,    &quot;queue_index_journal_write_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_index_write_count&quot;: 0,    &quot;queue_index_write_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_index_read_count&quot;: 3,    &quot;queue_index_read_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_file_handle_open_attempt_count&quot;: 73,    &quot;io_file_handle_open_attempt_count_details&quot;: {        &quot;rate&quot;: 0    },    &quot;io_file_handle_open_attempt_avg_time&quot;: 0.031643835616438354,    &quot;io_file_handle_open_attempt_avg_time_details&quot;: {        &quot;rate&quot;: 0    },    &quot;connection_created&quot;: 0,    &quot;connection_created_details&quot;: {        &quot;rate&quot;: 0    },    &quot;connection_closed&quot;: 0,    &quot;connection_closed_details&quot;: {        &quot;rate&quot;: 0    },    &quot;channel_created&quot;: 0,    &quot;channel_created_details&quot;: {        &quot;rate&quot;: 0    },    &quot;channel_closed&quot;: 0,    &quot;channel_closed_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_declared&quot;: 0,    &quot;queue_declared_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_created&quot;: 0,    &quot;queue_created_details&quot;: {        &quot;rate&quot;: 0    },    &quot;queue_deleted&quot;: 0,    &quot;queue_deleted_details&quot;: {        &quot;rate&quot;: 0    },    &quot;cluster_links&quot;: [],    &quot;metrics_gc_queue_length&quot;: {        &quot;connection_closed&quot;: 0,        &quot;channel_closed&quot;: 0,        &quot;consumer_deleted&quot;: 0,        &quot;exchange_deleted&quot;: 0,        &quot;queue_deleted&quot;: 0,        &quot;vhost_deleted&quot;: 0,        &quot;node_node_deleted&quot;: 0,        &quot;channel_consumer_deleted&quot;: 0    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息队列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 消息队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据相关概念</title>
      <link href="/2019/07/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/07/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据基础"><a href="#大数据基础" class="headerlink" title="大数据基础"></a>大数据基础</h1><h2 id="数据采集工具"><a href="#数据采集工具" class="headerlink" title="数据采集工具"></a>数据采集工具</h2><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><pre><code>Sqoop,主要用于在Hadoop(Hive)与传统数据库(Mysql,Oracle等)之间进行数据传递,可以将传统关系型数据库中的数据导入到Hadoop的HDFS中,也可以将HDFS的数据导入到关系型数据库中</code></pre><h3 id="Kettle"><a href="#Kettle" class="headerlink" title="Kettle"></a>Kettle</h3><pre><code>Kettle数据抽取工具(ETL),数据抽取,转换,加载,也可将数据从数据库抽取后存入Hadoop中</code></pre><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><pre><code>Flume是用于流数据存储到HDFS,如微博产生的推文,或者登录Web服务器的文件</code></pre><h2 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h2><pre><code>流数据是指一组顺序,大量,快速,连续到达的数据序列,数据流是一个随着时间延续而无限增长的动态数据集合.特点:1.数据实时到达2.数据到达次序独立,不受应用系统所控制3.数据量大且无法估算最大值4.数据一旦处理,除非特意保存,否则不能再取出处理,或者再次提取数据代价昂贵</code></pre><h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><pre><code>特点:1.有界:批处理数据集代表数据的有限集合2.持久:数据通常始终存储在某种类型的持久存储位置中3.大量:批处理操作通常是处理极为海量数据集的唯一方法</code></pre><h2 id="大数据处理框架"><a href="#大数据处理框架" class="headerlink" title="大数据处理框架"></a>大数据处理框架</h2><h3 id="批处理框架"><a href="#批处理框架" class="headerlink" title="批处理框架"></a>批处理框架</h3><h4 id="Apache-Hadoop"><a href="#Apache-Hadoop" class="headerlink" title="Apache Hadoop"></a>Apache Hadoop</h4><pre><code>不实时,数据存储在硬盘上,数据处理有延时,但是因为数据存储在硬盘上可以存储长期数据</code></pre><h3 id="流处理框架"><a href="#流处理框架" class="headerlink" title="流处理框架"></a>流处理框架</h3><h4 id="Apache-Storm-数据存储在内存中-实时读取"><a href="#Apache-Storm-数据存储在内存中-实时读取" class="headerlink" title="Apache Storm(数据存储在内存中,实时读取)"></a>Apache Storm(数据存储在内存中,实时读取)</h4><pre><code>完全实时,处理数据为毫秒级别</code></pre><h4 id="Apache-Samza"><a href="#Apache-Samza" class="headerlink" title="Apache Samza"></a>Apache Samza</h4><pre><code>Samza是一种与Kafka消息系统紧密绑定的流处理框架,适用于已经实现Hadoop与Kafka的项目中</code></pre><h3 id="混合框架-一站式框架"><a href="#混合框架-一站式框架" class="headerlink" title="混合框架 一站式框架"></a>混合框架 一站式框架</h3><h4 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h4><pre><code>Flink主要设计为流处理批处理时从持久存储中读取有边界的数据集,将批处理数据传入到流处理,当作流处理来处理批处理数据</code></pre><h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h4><pre><code>准实时,处理数据为秒级别Spark其主要设计为批处理,流处理主要由Spark Streaming实现,在流处理模式中,将流数据缓存一小部分后,进行处理,即微批处理模式可以与Hadoop集成替换Hadoop中MapReduce计算引擎与MapReduce相比可以用更快的速度处理相同的数据集,并且易于编写</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos搭建Jenkins环境</title>
      <link href="/2019/01/01/Centos%E6%90%AD%E5%BB%BAJenkins%E7%8E%AF%E5%A2%83/"/>
      <url>/2019/01/01/Centos%E6%90%AD%E5%BB%BAJenkins%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><blockquote><p>Jenkins环境的搭建会有易于开发以及测试环境的快速构建，其中我刚开始尝试了使用Docker搭建Jenkins环境，但是因为刚接触Docker在使用过程中遇到了Docker重置的情况所以在此为了快速应用到实际项目中去使用普通的搭建Jenkins的方法</p></blockquote><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ol><li>Centos7版本</li><li>JDK8</li><li>Jenkins2.7.3版本</li></ol><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><blockquote><p>此处省略安装JDK的步骤</p></blockquote><ol><li><p>安装Jenkins</p><p> 1.wget <a href="http://pkg.jenkins-ci.org/redhat-stable/jenkins-2.7.3-1.1.noarch.rpm" target="_blank" rel="external">http://pkg.jenkins-ci.org/redhat-stable/jenkins-2.7.3-1.1.noarch.rpm</a></p><p> 2.rpm -ivh jenkins-2.7.3-1.1.noarch.rpm</p></li></ol><blockquote><p>Jenkins包下载可能比较慢，所以建议使用以上链接用迅雷下载</p></blockquote><ol><li><p>设置Jenkins的端口号</p><p> 进入/etc/sysconfig/文件夹打开jenkins文件修改默认8080端口号</p></li><li><p>开始Jenkins服务</p><p> service jenkins start/stop/restart</p></li><li><p>打开Jenkins</p><p> 直接在浏览器上访问ip加设置的端口号，进入Jenkins的登录页面，登录页面中会提示密码在哪个文件中，取出密码点击Enter就可进入Jenkins页面了。</p></li><li><p>配置Jenkins以及注意事项</p><p> 1.首先安装Jenkins需要的插件直接选择推荐安装，安装完成后如果有未完成的可以先通过，后期再安装。</p><p> 2.安装完成后会提示新建用户，这时可以注册个用户，请记住这里的用户名和密码。</p><p> 3.登录后，进入系统设置-&gt;Configure Global Security将”Jenkins专有用户数据库”选中，然后允许用户注册，点击保存，然后可以试一下注销，然后是否可以用刚才注册的用户进行登录。</p><p> 4.再次登录后，点击”新建项目”构建一个自由风格的软件项目。</p><p> 5.然后选中源码管理模块，设置管理为GIT，然后填入Repository URL字段，并且需要设置下GIT的分支模块。</p><p> 6.这时需要再回到刚才的搭建Jenkins的服务器，然后使用ssh-keygen -t rsa的命令生成密钥。</p><p> 7.将生成好的公钥提供给对应的GIT管理平台，然后将私钥填入Jenkins的私钥部分。点击保存。</p><p> 8.这时Jenkins新建的项目就可以构建了，然后点击”一键构建”就可以构建整个项目，然后访问对应的项目端口就行了。</p></li></ol><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cnblogs.com/reblue520/p/7130914.html" target="_blank" rel="external">https://www.cnblogs.com/reblue520/p/7130914.html</a><br><a href="https://www.cnblogs.com/loveyouyou616/p/8714544.html" target="_blank" rel="external">https://www.cnblogs.com/loveyouyou616/p/8714544.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA记录操作日志步骤</title>
      <link href="/2018/09/17/%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/"/>
      <url>/2018/09/17/%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><blockquote><p>系统日志不论是在日常的管理还是维护中都会起到很大的作用,但是在日志的记录中通常会存在很多的问题</p></blockquote><ol><li>日志记录的不规范性</li><li>日志记录的重复性</li><li>日志记录的难分类</li></ol><blockquote><p>目前日志主要记录的有三方面</p></blockquote><ol><li>请求的入参,出参</li><li>关于业务上的操作</li><li>异常日常日志的打印</li></ol><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="1-记录请求的出参入参"><a href="#1-记录请求的出参入参" class="headerlink" title="1.记录请求的出参入参"></a>1.记录请求的出参入参</h3><blockquote><p>记录出参入参这是日志记录最好操作的一部分,而这里会存在一定的重复性,因为每个请求都需要记录,这是重复操作,完全可以使用Spring AOP进行入参和出参的记录</p></blockquote><pre><code>import org.apache.log4j.Logger;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import org.springframework.web.context.request.RequestContextHolder;import org.springframework.web.context.request.ServletRequestAttributes;import javax.servlet.http.HttpServletRequest;import java.util.Arrays;/** * Web层日志切面 */@Aspect    //这里使用@Aspect注解方式设置AOP@Order(5)  //值越小,越先加载@Componentpublic class WebLogAspect {    private Logger logger = Logger.getLogger(getClass());    ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;&gt;();    //这里@Pointcut设置切点可以设置为Controller层的地址    @Pointcut(&quot;execution(public * com.training..*.*Controller(..))&quot;)    public void webLog(){}    //@Before指在切点方法之前执行,也就是在Controller层方法执行之前执行,这里可以通过JoinPoint获取一些有关方法的信息,在这里也可以修改参数的值    //@Before()括号里设置的是切点方法的名称    @Before(&quot;webLog()&quot;)    public void doBefore(JoinPoint joinPoint) throws Throwable {        startTime.set(System.currentTimeMillis());        // 接收到请求，记录请求内容        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();        HttpServletRequest request = attributes.getRequest();        // 记录下请求内容        logger.info(&quot;URL : &quot; + request.getRequestURL().toString());        logger.info(&quot;HTTP_METHOD : &quot; + request.getMethod());        logger.info(&quot;IP : &quot; + request.getRemoteAddr());        logger.info(&quot;CLASS_METHOD : &quot; + joinPoint.getSignature().getDeclaringTypeName() + &quot;.&quot; + joinPoint.getSignature().getName());        logger.info(&quot;ARGS : &quot; + Arrays.toString(joinPoint.getArgs()));    }    @AfterReturning(returning = &quot;ret&quot;, pointcut = &quot;webLog()&quot;)    public void doAfterReturning(Object ret) throws Throwable {        // 处理完请求，返回内容        logger.info(&quot;RESPONSE : &quot; + ret);        logger.info(&quot;SPEND TIME : &quot; + (System.currentTimeMillis() - startTime.get()));    }}</code></pre><h3 id="2-记录操作日志"><a href="#2-记录操作日志" class="headerlink" title="2.记录操作日志"></a>2.记录操作日志</h3><blockquote><p>在系统中可能会有很多的增删改查或是会涉及到一些业务操作,这时候我们需要记录一下操作的入口,在此还可以记录下操作的类型,或是业务的名称.不过在就操作日志前需要构建日志的基础部件</p></blockquote><h3 id="日志对象"><a href="#日志对象" class="headerlink" title="日志对象"></a>日志对象</h3><pre><code>import java.util.Date;/** * 操作日志 */public class OperationLog extends Base {    private static final long serialVersionUID = 1L;    /**     * 日志类型     */    private String  logtype;    /**     * 日志名称     */    private String  logname;    /**     * 用户id     */    private Integer userid;    /**     * 类名称     */    private String  classname;    /**     * 方法名称     */    private String  method;    /**     * 创建时间     */    private Date    createtime;    /**     * 是否成功     */    private String  succeed;    /**     * 备注     */    private String  message;    public Integer getId() {        return id;    }    public void setId(Integer id) {        this.id = id;    }    public String getLogtype() {        return logtype;    }    public void setLogtype(String logtype) {        this.logtype = logtype;    }    public String getLogname() {        return logname;    }    public void setLogname(String logname) {        this.logname = logname;    }    public Integer getUserid() {        return userid;    }    public void setUserid(Integer userid) {        this.userid = userid;    }    public String getClassname() {        return classname;    }    public void setClassname(String classname) {        this.classname = classname;    }    public String getMethod() {        return method;    }    public void setMethod(String method) {        this.method = method;    }    public Date getCreatetime() {        return createtime;    }    public void setCreatetime(Date createtime) {        this.createtime = createtime;    }    public String getSucceed() {        return succeed;    }    public void setSucceed(String succeed) {        this.succeed = succeed;    }    public String getMessage() {        return message;    }    public void setMessage(String message) {        this.message = message;    }}</code></pre><h3 id="日志对象创建工厂"><a href="#日志对象创建工厂" class="headerlink" title="日志对象创建工厂"></a>日志对象创建工厂</h3><pre><code>import com.stylefeng.guns.common.constant.state.LogSucceed;import com.stylefeng.guns.common.constant.state.LogType;import com.stylefeng.guns.common.persistence.model.LoginLog;import com.stylefeng.guns.common.persistence.model.OperationLog;import java.util.Date;/** * 日志对象创建工厂 */public class LogFactory {    /**     * 创建操作日志     */    public static OperationLog createOperationLog(LogType logType, Integer userId, String bussinessName, String clazzName, String methodName, String msg, LogSucceed succeed) {        OperationLog operationLog = new OperationLog();        operationLog.setLogtype(logType.getMessage());        operationLog.setLogname(bussinessName);        operationLog.setUserid(userId);        operationLog.setClassname(clazzName);        operationLog.setMethod(methodName);        operationLog.setCreatetime(new Date());        operationLog.setSucceed(succeed.getMessage());        operationLog.setMessage(msg);        return operationLog;    }    /**     * 创建登录日志     */    public static LoginLog createLoginLog(LogType logType, Integer userId, String msg,String ip) {        LoginLog loginLog = new LoginLog();        loginLog.setLogname(logType.getMessage());        loginLog.setUserid(userId);        loginLog.setCreatetime(new Date());        loginLog.setSucceed(LogSucceed.SUCCESS.getMessage());        loginLog.setIp(ip);        loginLog.setMessage(msg);        return loginLog;    }}</code></pre><h3 id="日志任务创建工厂"><a href="#日志任务创建工厂" class="headerlink" title="日志任务创建工厂"></a>日志任务创建工厂</h3><blockquote><p>日志任务创建工厂的作用是将日志记录存储到数据库中</p></blockquote><pre><code>import com.stylefeng.guns.common.constant.state.LogSucceed;import com.stylefeng.guns.common.constant.state.LogType;import com.stylefeng.guns.common.persistence.dao.LoginLogMapper;import com.stylefeng.guns.common.persistence.dao.OperationLogMapper;import com.stylefeng.guns.common.persistence.model.LoginLog;import com.stylefeng.guns.common.persistence.model.OperationLog;import com.stylefeng.guns.core.log.LogManager;import com.stylefeng.guns.core.util.SpringContextHolder;import com.stylefeng.guns.core.util.ToolUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.TimerTask;/** * 日志操作任务创建工厂 */public class LogTaskFactory {    private static Logger logger  = LoggerFactory.getLogger(LogManager.class);    //LoginLogMapper记录登录登出日志    private static LoginLogMapper loginLogMapper = SpringContextHolder.getBean(LoginLogMapper.class);    //OperationLogMapper记录操作日志            private static OperationLogMapper operationLogMapper = SpringContextHolder.getBean(OperationLogMapper.class);    public static TimerTask loginLog(final Integer userId, final String ip) {        return new TimerTask() {            @Override            public void run() {                try {                    LoginLog loginLog = LogFactory.createLoginLog(LogType.LOGIN, userId, null, ip);                    loginLogMapper.insert(loginLog);                } catch (Exception e) {                    logger.error(&quot;创建登录日志异常!&quot;, e);                }            }        };    }    public static TimerTask loginLog(final String username, final String msg, final String ip) {        return new TimerTask() {            @Override            public void run() {                LoginLog loginLog = LogFactory.createLoginLog(                        LogType.LOGIN_FAIL, null, &quot;账号:&quot; + username + &quot;,&quot; + msg, ip);                try {                    loginLogMapper.insert(loginLog);                } catch (Exception e) {                    logger.error(&quot;创建登录失败异常!&quot;, e);                }            }        };    }    public static TimerTask exitLog(final Integer userId, final String ip) {        return new TimerTask() {            @Override            public void run() {                LoginLog loginLog = LogFactory.createLoginLog(LogType.EXIT, userId, null,ip);                try {                    loginLogMapper.insert(loginLog);                } catch (Exception e) {                    logger.error(&quot;创建退出日志异常!&quot;, e);                }            }        };    }    public static TimerTask bussinessLog(final Integer userId, final String bussinessName, final String clazzName, final String methodName, final String msg) {        return new TimerTask() {            @Override            public void run() {                OperationLog operationLog = LogFactory.createOperationLog(                        LogType.BUSSINESS, userId, bussinessName, clazzName, methodName, msg, LogSucceed.SUCCESS);                try {                    operationLogMapper.insert(operationLog);                } catch (Exception e) {                    logger.error(&quot;创建业务日志异常!&quot;, e);                }            }        };    }    public static TimerTask exceptionLog(final Integer userId, final Exception exception) {        return new TimerTask() {            @Override            public void run() {                String msg = ToolUtil.getExceptionMsg(exception);                OperationLog operationLog = LogFactory.createOperationLog(                        LogType.EXCEPTION, userId, &quot;&quot;, null, null, msg, LogSucceed.FAIL);                try {                    operationLogMapper.insert(operationLog);                } catch (Exception e) {                    logger.error(&quot;创建异常日志异常!&quot;, e);                }            }        };    }}</code></pre><h3 id="记录操作日志"><a href="#记录操作日志" class="headerlink" title="记录操作日志"></a>记录操作日志</h3><blockquote><p>这一步是最关键的一环</p><p>原理:通过自定义的注解@BussinessLog(可以任意命名),里面定义了业务的名称,被修改的实体的唯一标识,字典(用于查找key的中文名称和字段的中文名称),然后通过AOP,拦截所有添加了@BussinessLog注解的方法,解析其注解里面的属性,然后记录到对应的操作日志表中,完成操作日志的记录</p></blockquote><h4 id="BussinessLog"><a href="#BussinessLog" class="headerlink" title="@BussinessLog"></a>@BussinessLog</h4><pre><code>import java.lang.annotation.*;/** * 标记需要做业务日志的方法 */@Inherited@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD})public @interface BussinessLog {    /**     * 业务的名称,例如:&quot;修改菜单&quot;     */    String value() default &quot;&quot;;    /**     * 被修改的实体的唯一标识,例如:菜单实体的唯一标识为&quot;id&quot;     */    String key() default &quot;id&quot;;    /**     * 字典(用于查找key的中文名称和字段的中文名称)     */    String dict() default &quot;SystemDict&quot;;}</code></pre><h4 id="BussinessLog注解拦截AOP"><a href="#BussinessLog注解拦截AOP" class="headerlink" title="@BussinessLog注解拦截AOP"></a>@BussinessLog注解拦截AOP</h4><pre><code>import com.stylefeng.guns.common.annotion.log.BussinessLog;import com.stylefeng.guns.common.constant.dictmap.base.AbstractDictMap;import com.stylefeng.guns.common.constant.dictmap.factory.DictMapFactory;import com.stylefeng.guns.core.log.LogManager;import com.stylefeng.guns.core.log.LogObjectHolder;import com.stylefeng.guns.core.log.factory.LogTaskFactory;import com.stylefeng.guns.core.shiro.ShiroKit;import com.stylefeng.guns.core.shiro.ShiroUser;import com.stylefeng.guns.core.support.HttpKit;import com.stylefeng.guns.core.util.Contrast;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.Signature;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import java.lang.reflect.Method;import java.util.Map;/** * 日志记录 */@Aspect@Componentpublic class LogAop {    private Logger log = LoggerFactory.getLogger(this.getClass());    @Pointcut(value = &quot;@annotation(com.stylefeng.guns.common.annotion.log.BussinessLog)&quot;)    public void cutService() {    }    @Around(&quot;cutService()&quot;)    public Object recordSysLog(ProceedingJoinPoint point) throws Throwable {        //先执行业务        Object result = point.proceed();        try {            handle(point);        } catch (Exception e) {            log.error(&quot;日志记录出错!&quot;, e);        }        return result;    }    private void handle(ProceedingJoinPoint point) throws Exception {        //获取拦截的方法名        Signature sig = point.getSignature();        MethodSignature msig = null;        if (!(sig instanceof MethodSignature)) {            throw new IllegalArgumentException(&quot;该注解只能用于方法&quot;);        }        msig = (MethodSignature) sig;        Object target = point.getTarget();        Method currentMethod = target.getClass().getMethod(msig.getName(), msig.getParameterTypes());        String methodName = currentMethod.getName();        //如果当前用户未登录，不做日志        ShiroUser user = ShiroKit.getUser();        if (null == user) {            return;        }        //获取拦截方法的参数        String className = point.getTarget().getClass().getName();        Object[] params = point.getArgs();        //获取操作名称        BussinessLog annotation = currentMethod.getAnnotation(BussinessLog.class);        String bussinessName = annotation.value();        String key = annotation.key();        String dictClass = annotation.dict();        StringBuilder sb = new StringBuilder();        for (Object param : params) {            sb.append(param);            sb.append(&quot; &amp; &quot;);        }        //如果涉及到修改,比对变化        String msg;        if (bussinessName.indexOf(&quot;修改&quot;) != -1 || bussinessName.indexOf(&quot;编辑&quot;) != -1) {            Object obj1 = LogObjectHolder.me().get();            Map&lt;String, String&gt; obj2 = HttpKit.getRequestParameters();            msg = Contrast.contrastObj(dictClass, key, obj1, obj2);        } else {            Map&lt;String, String&gt; parameters = HttpKit.getRequestParameters();            AbstractDictMap dictMap = DictMapFactory.createDictMap(dictClass);            msg = Contrast.parseMutiKey(dictMap,key,parameters);        }        LogManager.me().executeLog(LogTaskFactory.bussinessLog(user.getId(), bussinessName, className, methodName, msg));    }}</code></pre><h4 id="BussinessLog使用实例"><a href="#BussinessLog使用实例" class="headerlink" title="@BussinessLog使用实例"></a>@BussinessLog使用实例</h4><pre><code>/** * 新增字典 @param dictValues 格式例如   &quot;1:启用;2:禁用;3:冻结&quot; */@BussinessLog(value = &quot;添加字典记录&quot;, key = &quot;dictName,dictValues&quot;, dict = com.stylefeng.guns.common.constant.Dict.DictMap)@RequestMapping(value = &quot;/add&quot;)@Permission(Const.ADMIN_NAME)@ResponseBodypublic Object add(String dictName, String dictValues) {    if (ToolUtil.isOneEmpty(dictName, dictValues)) {        throw new BussinessException(BizExceptionEnum.REQUEST_NULL);    }    dictService.addDict(dictName, dictValues);    return SUCCESS_TIP;}</code></pre><h3 id="3-记录异常日志"><a href="#3-记录异常日志" class="headerlink" title="3.记录异常日志"></a>3.记录异常日志</h3><blockquote><p>记录异常日志其实也是一个重复式的过程,这也可以通过统一的处理来记录异常抛出的日志</p></blockquote><pre><code>import com.stylefeng.guns.common.constant.tips.ErrorTip;import com.stylefeng.guns.common.exception.BizExceptionEnum;import com.stylefeng.guns.common.exception.BussinessException;import com.stylefeng.guns.common.exception.InvalidKaptchaException;import com.stylefeng.guns.core.log.LogManager;import com.stylefeng.guns.core.log.factory.LogTaskFactory;import com.stylefeng.guns.core.shiro.ShiroKit;import org.apache.shiro.authc.AuthenticationException;import org.apache.shiro.authc.CredentialsException;import org.apache.shiro.authc.DisabledAccountException;import org.apache.shiro.session.InvalidSessionException;import org.apache.shiro.session.UnknownSessionException;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.http.HttpStatus;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.ResponseStatus;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.lang.reflect.UndeclaredThrowableException;import static com.stylefeng.guns.core.support.HttpKit.getIp;import static com.stylefeng.guns.core.support.HttpKit.getRequest;/** * 全局的的异常拦截器（拦截所有的控制器）（带有@RequestMapping注解的方法上都会拦截） */@ControllerAdvicepublic class GlobalExceptionHandler {    private Logger log = LoggerFactory.getLogger(this.getClass());    /**     * 拦截业务异常     */    @ExceptionHandler(BussinessException.class)    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)    @ResponseBody    public ErrorTip notFount(BussinessException e) {        LogManager.me().executeLog(LogTaskFactory.exceptionLog(ShiroKit.getUser().getId(), e));        getRequest().setAttribute(&quot;tip&quot;, e.getMessage());        log.error(&quot;业务异常:&quot;, e);        return new ErrorTip(e.getCode(), e.getMessage());    }    /**     * 用户未登录     */    @ExceptionHandler(AuthenticationException.class)    @ResponseStatus(HttpStatus.UNAUTHORIZED)    public String unAuth(AuthenticationException e) {        log.error(&quot;用户未登陆：&quot;, e);        return &quot;/login.html&quot;;    }    /**     * 账号被冻结     */    @ExceptionHandler(DisabledAccountException.class)    @ResponseStatus(HttpStatus.UNAUTHORIZED)    public String accountLocked(DisabledAccountException e, Model model) {        String username = getRequest().getParameter(&quot;username&quot;);        LogManager.me().executeLog(LogTaskFactory.loginLog(username, &quot;账号被冻结&quot;, getIp()));        model.addAttribute(&quot;tips&quot;, &quot;账号被冻结&quot;);        return &quot;/login.html&quot;;    }    /**     * 账号密码错误     */    @ExceptionHandler(CredentialsException.class)    @ResponseStatus(HttpStatus.UNAUTHORIZED)    public String credentials(CredentialsException e, Model model) {        String username = getRequest().getParameter(&quot;username&quot;);        LogManager.me().executeLog(LogTaskFactory.loginLog(username, &quot;账号密码错误&quot;, getIp()));        model.addAttribute(&quot;tips&quot;, &quot;账号密码错误&quot;);        return &quot;/login.html&quot;;    }    /**     * 验证码错误     */    @ExceptionHandler(InvalidKaptchaException.class)    @ResponseStatus(HttpStatus.BAD_REQUEST)    public String credentials(InvalidKaptchaException e, Model model) {        String username = getRequest().getParameter(&quot;username&quot;);        LogManager.me().executeLog(LogTaskFactory.loginLog(username, &quot;验证码错误&quot;, getIp()));        model.addAttribute(&quot;tips&quot;, &quot;验证码错误&quot;);        return &quot;/login.html&quot;;    }    /**     * 无权访问该资源     */    @ExceptionHandler(UndeclaredThrowableException.class)    @ResponseStatus(HttpStatus.UNAUTHORIZED)    @ResponseBody    public ErrorTip credentials(UndeclaredThrowableException e) {        getRequest().setAttribute(&quot;tip&quot;, &quot;权限异常&quot;);        log.error(&quot;权限异常!&quot;, e);        return new ErrorTip(BizExceptionEnum.NO_PERMITION);    }    /**     * 拦截未知的运行时异常     */    @ExceptionHandler(RuntimeException.class)    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)    @ResponseBody    public ErrorTip notFount(RuntimeException e) {        LogManager.me().executeLog(LogTaskFactory.exceptionLog(ShiroKit.getUser().getId(), e));        getRequest().setAttribute(&quot;tip&quot;, &quot;服务器未知运行时异常&quot;);        log.error(&quot;运行时异常:&quot;, e);        return new ErrorTip(BizExceptionEnum.SERVER_ERROR);    }    /**     * session失效的异常拦截     */    @ExceptionHandler(InvalidSessionException.class)    @ResponseStatus(HttpStatus.BAD_REQUEST)    public String sessionTimeout(InvalidSessionException e, Model model, HttpServletRequest request, HttpServletResponse response) {        model.addAttribute(&quot;tips&quot;, &quot;session超时&quot;);        assertAjax(request, response);        return &quot;/login.html&quot;;    }    /**     * session异常     */    @ExceptionHandler(UnknownSessionException.class)    @ResponseStatus(HttpStatus.BAD_REQUEST)    public String sessionTimeout(UnknownSessionException e, Model model, HttpServletRequest request, HttpServletResponse response) {        model.addAttribute(&quot;tips&quot;, &quot;session超时&quot;);        assertAjax(request, response);        return &quot;/login.html&quot;;    }    private void assertAjax(HttpServletRequest request, HttpServletResponse response) {        if (request.getHeader(&quot;x-requested-with&quot;) != null                &amp;&amp; request.getHeader(&quot;x-requested-with&quot;).equalsIgnoreCase(&quot;XMLHttpRequest&quot;)) {            //如果是ajax请求响应头会有，x-requested-with            response.setHeader(&quot;sessionstatus&quot;, &quot;timeout&quot;);//在响应头设置session状态        }    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA实现冒泡排序</title>
      <link href="/2018/06/06/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"/>
      <url>/2018/06/06/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<p>原理：比较两个相邻的元素，将值大的元素交换至右端。</p><p>思路：依次比较相邻的两个数，将小数放在前面，大数放在后面。即在第一趟：首先比较第1个和第2个数，将小数放前，大数放后。然后比较第2个数和第3个数，将小数放前，大数放后，如此继续，直至比较最后两个数，将小数放前，大数放后。重复第一趟步骤，直至全部排序完成。</p><p>第一趟比较完成后，最后一个数一定是数组中最大的一个数，所以第二趟比较的时候最后一个数不参与比较；</p><p>第二趟比较完成后，倒数第二个数也一定是数组中第二大的数，所以第三趟比较的时候最后两个数不参与比较；</p><p>依次类推，每一趟比较次数-1；</p><p>……</p><p>举例说明：要排序数组：int[] arr={6,3,8,2,9,1};   </p><p>第一趟排序：</p><p>　　　　第一次排序：6和3比较，6大于3，交换位置：  3  6  8  2  9  1</p><p>　　　　第二次排序：6和8比较，6小于8，不交换位置：3  6  8  2  9  1</p><p>　　　　第三次排序：8和2比较，8大于2，交换位置：  3  6  2  8  9  1</p><p>　　　　第四次排序：8和9比较，8小于9，不交换位置：3  6  2  8  9  1</p><p>　　　　第五次排序：9和1比较：9大于1，交换位置：  3  6  2  8  1  9</p><p>　　　　第一趟总共进行了5次比较， 排序结果：      3  6  2  8  1  9</p><hr><p>第二趟排序：</p><p>　　　　第一次排序：3和6比较，3小于6，不交换位置：3  6  2  8  1  9</p><p>　　　　第二次排序：6和2比较，6大于2，交换位置：  3  2  6  8  1  9</p><p>　　　　第三次排序：6和8比较，6大于8，不交换位置：3  2  6  8  1  9</p><p>　　　　第四次排序：8和1比较，8大于1，交换位置：  3  2  6  1  8  9</p><p>　　　　第二趟总共进行了4次比较， 排序结果：      3  2  6  1  8  9</p><hr><p>第三趟排序：</p><p>　　　　第一次排序：3和2比较，3大于2，交换位置：  2  3  6  1  8  9</p><p>　　　　第二次排序：3和6比较，3小于6，不交换位置：2  3  6  1  8  9</p><p>　　　　第三次排序：6和1比较，6大于1，交换位置：  2  3  1  6  8  9</p><p>　　　　第二趟总共进行了3次比较， 排序结果：         2  3  1  6  8  9</p><hr><p>第四趟排序：</p><p>　　　　第一次排序：2和3比较，2小于3，不交换位置：2  3  1  6  8  9</p><p>　　　　第二次排序：3和1比较，3大于1，交换位置：  2  1  3  6  8  9</p><p>　　　　第二趟总共进行了2次比较， 排序结果：        2  1  3  6  8  9</p><hr><p>第五趟排序：</p><p>　　　　第一次排序：2和1比较，2大于1，交换位置：  1  2  3  6  8  9</p><p>　　　　第二趟总共进行了1次比较， 排序结果：  1  2  3  6  8  9</p><hr><p>最终结果：1  2  3  6  8  9</p><hr><p>由此可见：N个数字要排序完成，总共进行N-1趟排序，每i趟的排序次数为(N-i)次，所以可以用双重循环语句，外层控制循环多少趟，内层控制每一趟的循环次数，即</p><pre><code>for(i=0;i&lt;a.length-1;i++){    for(j=0;j&lt;a.length-1-1;j++){    }}</code></pre><p>冒泡排序的优点：每进行一趟排序，就会少比较一次，因为每进行一趟排序都会找出一个较大值。如上例：第一趟比较之后，排在最后的一个数一定是最大的一个数，第二趟排序的时候，只需要比较除了最后一个数以外的其他的数，同样也能找出一个最大的数排在参与第二趟比较的数后面，第三趟比较的时候，只需要比较除了最后两个数以外的其他的数，以此类推……也就是说，没进行一趟比较，每一趟少比较一次，一定程度上减少了算法的量。</p><p>用时间复杂度来说：</p><p>　　1.如果我们的数据正序，只需要走一趟即可完成排序。所需的比较次数C和记录移动次数M均达到最小值，即：Cmin=n-1;Mmin=0;所以，冒泡排序最好的时间复杂度为O(n)。</p><p>　　2.如果很不幸我们的数据是反序的，则需要进行n-1趟排序。每趟排序要进行n-i次比较(1≤i≤n-1)，且每次比较都必须移动记录三次来达到交换记录位置。在这种情况下，比较和移动次数均达到最大值：冒泡排序的最坏时间复杂度为：O(n2) 。</p><p>综上所述：冒泡排序总的平均时间复杂度为：O(n2) 。</p><p>代码实现：</p><pre><code>int[] a = {6,1,3,4,2,5};for (int i = 0; i &lt;a.length-1 ; i++) {    for (int j = 0; j &lt;a.length-1-i ; j++) {       if(a[j]&gt;a[j+1]){           int m = a[j];           a[j] = a[j+1];           a[j+1] = m;       }    }}System.out.println(Arrays.toString(a));</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一次的记录</title>
      <link href="/2018/04/16/one/"/>
      <url>/2018/04/16/one/</url>
      
        <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;作为一个程序员也是第一次接触个人博客的搭建，搭建博客的本意是为了能够将日常工作中遇到的问题记录下来更好的分享给每一个技术人，与此同时我更加想将生活中的点点滴滴也记录其中，这是作为一个程序员的生活记录。因为当下的朋友圈，微博对于技术人都太聒噪，但作为个人博客则可以更加自由的记录自己的心境。在此立个Flag，希望可以坚持下去不断的更新完善自己的博客。</p>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
